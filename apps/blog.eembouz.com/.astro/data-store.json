[["Map",1,2,9,10,62,63,71,72],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.16.6","content-config-digest","6c4168f31707a980","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://blog.eembouz.com/\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"prefetch\":false,\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/noop\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":false},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{\"light\":\"github-light\",\"dark\":\"github-dark\"},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[null],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"i18n\":{\"defaultLocale\":\"fr\",\"locales\":[\"en\",\"fr\"],\"routing\":{\"prefixDefaultLocale\":false,\"redirectToDefaultLocale\":true,\"fallbackType\":\"redirect\"}},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}","taxonomies",["Map",11,12,17,18,23,24,29,30,35,36,40,41,46,47,52,53,57,58],"architecture-design",{"id":11,"data":13,"filePath":15,"digest":16},{"name":14},"Design architecture","src/data/taxonomies/architecture-design.json","08799596a8c6e694","best-practices-clean-code",{"id":17,"data":19,"filePath":21,"digest":22},{"name":20},"best-practices / Clean code","src/data/taxonomies/best-practices-clean-code.json","d70ec7d6a08b7424","deep-learning",{"id":23,"data":25,"filePath":27,"digest":28},{"name":26},"Deep learning","src/data/taxonomies/deep-learning.json","fbf840517b1dcaee","design",{"id":29,"data":31,"filePath":33,"digest":34},{"name":32},"Design systems","src/data/taxonomies/design.json","2cbf2cbe6dd8265f","devops",{"id":35,"data":37,"filePath":38,"digest":39},{"name":35},"src/data/taxonomies/devops.json","1997f4f58e570e91","digital-ux-ui-design",{"id":40,"data":42,"filePath":44,"digest":45},{"name":43},"Digital / UX/UI Design","src/data/taxonomies/digital-ux-ui-design.json","3191f7292d0b051b","graphic-design",{"id":46,"data":48,"filePath":50,"digest":51},{"name":49},"Graphic Design","src/data/taxonomies/graphic-design.json","7bb9a482bad2297a","ia",{"id":52,"data":54,"filePath":55,"digest":56},{"name":52},"src/data/taxonomies/ia.json","05c1796405534d48","programming",{"id":57,"data":59,"filePath":60,"digest":61},{"name":57},"src/data/taxonomies/programming.json","0efd4594458f1f34","authors",["Map",64,65],"elysee",{"id":64,"data":66,"filePath":69,"digest":70},{"name":67,"portfolio":68},"Elysee","https://eembouz.com/","src/data/authors/elysee.json","76bba3a72d597752","blog",["Map",73,74,90,91,103,104,117,118,130,131,143,144,156,157,170,171,181,182,194,195,207,208,219,220,231,232,242,243],"convolutional-neural-networks",{"id":73,"data":75,"body":86,"filePath":87,"digest":88,"deferredRender":89},{"title":76,"contentType":77,"isDraft":78,"taxonomies":79,"thumbnail":82,"summary":83,"pubDate":84,"author":85},"Convolutional neural networks",1,false,[80,81],{"id":52,"collection":9},{"id":23,"collection":9},"/iclh-diagram-convolutional-neural-networks.png","Un réseau neuronal convolutionnel (Convolutional Neural Network, ou CNN) est un type d’algorithme d’apprentissage profond supervisé (deep learning) qui confère à un ordinateur une forme de « vision » : il peut ainsi analyser et reconnaître des objets du monde visuel.",["Date","2025-08-12T00:00:00.000Z"],{"id":64,"collection":62},"**Du sport à la médecine, en passant par les transports et la sécurité…**  \r\nVous êtes-vous déjà demandé comment une caméra de surveillance parvient à reconnaître un visage dans une foule, comment un scanner médical détecte une tumeur dissimulée, ou comment un système suit la trajectoire d'un train avec une précision millimétrique ?\r\n\r\n## Introduction\r\n\r\nUn **réseau neuronal convolutionnel** (_Convolutional Neural Network_, ou CNN) est un type d'algorithme d'apprentissage profond supervisé (_deep learning_) qui confère à un ordinateur une forme de « vision » : il peut ainsi analyser et reconnaître des objets du monde visuel.\r\n\r\nLe **réseau neuronal convolutionnel** (_Convolutional Neural Network_, CNN), par analogie, se réfère au fonctionnement des réseaux neuronaux biologiques et constitue, par conséquent, une extension du réseau neuronal artificiel (_Artificial Neural Network_, ANN). Il trouve ses applications principalement dans la reconnaissance et l'analyse d'images.\r\n\r\nEn s'appuyant sur les principes de l'algèbre linéaire, et plus particulièrement sur la manipulation de matrices, les réseaux neuronaux convolutionnels appliquent des opérations de convolution et de transformation afin de détecter et extraire des motifs pertinents au sein d'une image.\r\n\r\n## Comment fonctionne le CNN\r\n\r\nLe fonctionnement d'un **réseau neuronal convolutionnel** (_Convolutional Neural Network_, CNN) repose sur une architecture composée de trois types de couches principales. Ces couches traitent en entrée (_input_) des données structurées spatialement, telles que des images, des spectrogrammes audio ou des séquences vidéo :\r\n\r\n- **Couche convolutionnelle** (_Convolutional layer_)\r\n- **Couche de sous-échantillonnage ou de pooling** (_Pooling layer_)\r\n- **Couche entièrement connectée** (_Fully connected layer_)\r\n\r\n## **Couche convolutionnelle (Convolutional layer)**\r\n\r\nLa couche convolutionnelle est le cœur principal du réseau de neurones, c'est là où la majorité des calculs sont effectués. Son fonctionnement nécessite des composants tels que :\r\n\r\n- Des données d'entrée (input data)\r\n- Un filtre (noyau ou kernel)\r\n- Une carte de caractéristiques (feature map)\r\n\r\n### Fonctionnement :\r\n\r\n- **Convolution** (produit scalaire)\r\n- **ReLU** (supprime les valeurs négatives)\r\n- **Pooling** (réduction de taille)\r\n- Répéter ces étapes\r\n- **Classification finale**\r\n\r\n**Données d'entrée (Input)** : Prenant pour entrée une image de taille 24×24 pixels, qui sera transformée en matrice de pixels 3D correspondant au nombre de pixels respectifs de l'image. Cela veut dire que l'input (la donnée d'entrée) est représenté en hauteur, largeur et profondeur qui correspondent aux caractéristiques RVB de l'image.\r\n\r\n**Filtre (noyau)** : Le filtre est une matrice de poids qui se déplace sur l'image afin d'en détecter les formes et caractéristiques. Ce processus est connu sous le nom d'opération de convolution.\r\n\r\n- Les contours\r\n- Les lignes\r\n- Les textures\r\n- Les formes géométriques\r\n- Des caractéristiques plus complexes dans les couches profondes\r\n\r\nSa taille peut varier, bien que par défaut elle soit de 3×3, il en existe des 5×5, ou 7×7 pixels. Le **champ récepteur** correspond à la zone de l'image d'entrée qui influence un neurone de sortie.\r\n\r\nLe filtre est placé sur une zone de l'image et le produit scalaire est calculé en fonction des pixels d'entrée et du filtre. Cette valeur est alors sauvegardée dans une carte de caractéristiques (feature map). Après cela, le filtre est alors déplacé d'un pas, répétant le processus jusqu'à ce que toute la surface des pixels de l'image soit traitée.\r\n\r\nCertains hyper-paramètres du filtre restent fixes durant le processus, par contre les **poids** sont ajustés afin de maximiser les performances via les opérations de rétropropagation et de descente de gradient. Il existe trois hyper-paramètres qui sont primordiaux dans l'obtention d'un résultat favorable et qui influencent le volume de sortie (output) ; ils doivent être définis bien avant le processus.\r\n\r\n1. **Number of filters (nombre de filtres)** : Affecte la profondeur de la sortie. Par exemple, trois filtres distincts produiraient trois cartes de caractéristiques différentes, créant ainsi une profondeur de trois. _Chaque filtre agit comme un détecteur spécialisé (contours, textures, formes) et produit sa propre carte de caractéristiques, ces cartes s'empilent pour former la profondeur de sortie._\r\n\r\n2. **Stride (pas ou foulée)** : Est la distance, ou le nombre de pixels, sur laquelle le noyau se déplace sur la matrice d'entrée. Bien que des valeurs de foulée de deux ou plus soient rares, une foulée plus grande donne une sortie plus petite. _Une foulée de 1 signifie que le filtre se déplace pixel par pixel (examen détaillé), tandis qu'une foulée de 2 fait sauter un pixel à chaque déplacement (examen plus rapide, image plus petite)._\r\n\r\n3. **Zero-padding (rembourrage zéro)** : Est généralement utilisé lorsque les filtres ne correspondent pas à l'image d'entrée. Cela met à zéro tous les éléments qui se trouvent en dehors de la matrice d'entrée, produisant une sortie plus grande ou de taille égale. Il existe trois types de rembourrage :\r\n\r\n   - **Rembourrage valide :** C'est ce qu'on appelle également l'absence de rembourrage. Dans ce cas, la dernière convolution est abandonnée si les dimensions ne s'alignent pas. _(Si les pièces du puzzle ne rentrent pas au bord, on les abandonne)_\r\n   - **Rembourrage identique :** Ce remplissage garantit que la couche de sortie a la même taille que la couche d'entrée. _(On ajoute des pièces vides autour pour que tout rentre parfaitement)_\r\n   - **Rembourrage complet :** Ce type de remplissage augmente la taille de la sortie en ajoutant des zéros à la bordure de l'entrée. _(On ajoute encore plus de pièces vides pour obtenir une image plus grande)_\r\n\r\n**Après l'opération de convolution, le CNN applique immédiatement la fonction ReLU** (**Re**ctified **L**inear **U**nit - Unité Linéaire Rectifiée). Cette fonction d'activation agit comme un filtre sélectif qui élimine les signaux faibles (valeurs négatives) pour ne conserver que les activations significatives (valeurs positives).\r\n\r\n\u003Cimg src=\"/Relu-activation-function.webp\" alt=\"Fonction d'activation ReLU\" />\r\n\r\n**Principe de fonctionnement :**\r\n\r\n- Valeur positive → conservée telle quelle\r\n- Valeur négative → transformée en zéro\r\n\r\n**Formule :** ReLU(x) = max(0, x)\r\n\r\n**Exemple pratique :**\r\nCarte de caractéristiques avant ReLU : `[-2, 5, -1, 8, -3, 4]`\r\nCarte de caractéristiques après ReLU : `[0, 5, 0, 8, 0, 4]`\r\n\r\n**Impact sur l'apprentissage :** L'introduction de cette non-linéarité est cruciale car elle permet au réseau de modéliser des relations complexes entre les caractéristiques. Sans ReLU, le CNN serait limité à des transformations linéaires et ne pourrait pas détecter des motifs sophistiqués comme les relations spatiales entre les éléments d'un visage (distance œil-nez, configuration bouche-joues) ou d'autres subtilités visuelles complexes.\r\n\r\nCette étape transforme donc un simple calcul mathématique en un véritable processus de reconnaissance intelligent.\r\n\r\n\u003Cimg\r\n  src=\"/iclh-diagram-convolutional-neural-networks.png\"\r\n  alt=\"Diagramme des réseaux neuronaux convolutionnels\"\r\n/>\r\n\r\n### Couche convolutive supplémentaire\r\n\r\nIl est fréquent qu'une hiérarchisation de couches soit appliquée pour analyser progressivement la complexité croissante d'une image. Cette architecture en cascade permet à la structure du CNN de s'adapter intelligemment : les couches ultérieures peuvent exploiter les informations des champs récepteurs des couches précédentes, créant ainsi une véritable hiérarchie de détection.\r\n\r\n**Principe de la hiérarchie des caractéristiques :**\r\n\r\n- **Couches initiales** : Détectent les caractéristiques de bas niveau (contours, lignes, textures simples)\r\n- **Couches intermédiaires** : Combinent ces éléments pour identifier des formes plus complexes (angles, courbes, motifs géométriques)\r\n- **Couches profondes** : Reconnaissent des parties d'objets spécifiques (roues, guidons, cadres)\r\n- **Couches finales** : Assemblent ces parties pour identifier l'objet complet (vélo, voiture, visage)\r\n\r\n**Exemple concret :** Prenons la reconnaissance d'un vélo. Le CNN procède par étapes :\r\n\r\n1. Détection des cercles et lignes droites\r\n2. Identification de formes circulaires (roues potentielles)\r\n3. Reconnaissance du guidon et du cadre\r\n4. Association finale : \"roues + guidon + cadre = vélo\"\r\n\r\nCette approche hiérarchique permet au réseau de construire une compréhension progressive de l'image, où chaque couche affine et enrichit l'analyse de la précédente. In fine, la couche convolutionnelle convertit l'image en valeurs numériques structurées, permettant au réseau neuronal d'interpréter et d'extraire les motifs pertinents pour la classification finale.\r\n\r\n\u003Cimg src=\"/hierarchy.png\" alt=\"Hiérarchie des caractéristiques CNN\" />\r\n\r\n## **Couche de sous-échantillonnage ou de pooling** (_Pooling layer_)\r\n\r\nDans cette étape du processus CNN, les informations collectées sont réduites et regroupées afin de diminuer la dimensionnalité des données. Dans le même processus que la couche convolutive, elle applique un filtre qui, à l'inverse de la couche convolutive, n'a pas de poids. Une fonction d'agrégation est appliquée aux valeurs du champ récepteur afin de remplir le tableau de sortie. **Cette fonction d'agrégation constitue le paramètre principal configurable qui détermine le type de pooling utilisé.**\r\n\r\n**Objectifs principaux :**\r\n\r\n- Réduire la taille des données\r\n- Diminuer le nombre de paramètres\r\n- Conserver les caractéristiques importantes\r\n\r\n**Les deux principaux types de pooling (fonctions d'agrégation) :**\r\n\r\n**1. Max Pooling (Regroupement maximal) :**\r\n\r\n- Fonction d'agrégation : `f(région) = max(valeurs)`\r\n- Sélectionne la **valeur maximale** dans chaque région du champ récepteur\r\n- Conserve les caractéristiques les plus saillantes\r\n- Plus couramment utilisé car il préserve les contours et détails importants\r\n\r\n**2. Average Pooling (Regroupement moyen) :**\r\n\r\n- Fonction d'agrégation : `f(région) = moyenne(valeurs)`\r\n- Calcule la **moyenne arithmétique** de toutes les valeurs dans le champ récepteur\r\n- Lisse les données en réduisant le bruit\r\n- Moins utilisé mais utile pour certaines applications spécifiques\r\n\r\n## Visualisation\r\n\r\n#### Région 2×2 organisée comme une matrice :\r\n\r\n\u003Cimg\r\n  src=\"/Sans-titre-2025-06-23-2323 (2).png\"\r\n  alt=\"Région 2x2 organisée en matrice\"\r\n/>\r\n\r\n## Calculs selon le type de pooling :\r\n\r\n**Max Pooling :**\r\n\r\n- On regarde toutes les valeurs : `1, 3, 2, 4`\r\n- On prend la **plus grande** : `max(1, 3, 2, 4) = 4`\r\n- **Résultat : 4**\r\n\r\n**Average Pooling :**\r\n\r\n- On additionne toutes les valeurs : `1 + 3 + 2 + 4 = 10`\r\n- On divise par le nombre de valeurs : `10 ÷ 4 = 2.5`\r\n- **Résultat : 2.5**\r\n\r\n## Visualisation du processus :\r\n\r\n\u003Cimg\r\n  src=\"/Sans-titre-2025-06-23-2323 (3).png\"\r\n  alt=\"Visualisation du processus de pooling\"\r\n/>\r\n\r\nLe filtre de pooling \"regarde\" cette région 2×2 et la **résume en une seule valeur** selon la fonction d'agrégation choisie.\r\n\r\nCes paramètres (type de fonction d'agrégation, taille du filtre, stride) sont configurables avant l'entraînement selon les besoins spécifiques du modèle, permettant d'adapter le comportement du pooling à la tâche de classification visée.\r\n\r\n## **Couche entièrement connectée** (_Fully connected layer_)\r\n\r\nCette couche effectue une classification en fonction des caractéristiques extraites des couches précédentes et de leurs filtres. Elle applique une fonction d'activation (généralement **softmax**) qui permet de convertir les données en **probabilités** : chaque classe possible reçoit un score entre 0 et 1, et la **somme de toutes les probabilités égale 1**.\r\n\r\n### Exemple :\r\n\r\nPour reconnaître des animaux :\r\n\r\n- Chat : 0.7 (70% de probabilité)\r\n- Chien : 0.2 (20% de probabilité)\r\n- Oiseau : 0.1 (10% de probabilité)\r\n- **Total : 0.7 + 0.2 + 0.1 = 1.0**\r\n\r\nLa classe avec la **plus haute probabilité** (ici \"Chat\" avec 0.7) est la prédiction finale.\r\n\r\n## Conclusion\r\n\r\nLes **réseaux neuronaux convolutionnels** (CNN) constituent une architecture d'apprentissage profond particulièrement efficace pour le traitement et l'analyse d'images. Leur fonctionnement repose sur trois composants principaux : les couches convolutionnelles qui extraient les caractéristiques, les couches de pooling qui réduisent la dimensionnalité, et les couches entièrement connectées qui effectuent la classification finale.\r\n\r\n### Architecture et performances\r\n\r\nCette structure hiérarchique permet aux CNN de détecter progressivement des motifs de plus en plus complexes, depuis les contours simples jusqu'aux objets complets. Les hyper-paramètres configurables (nombre de filtres, stride, padding) offrent une flexibilité d'adaptation selon les besoins spécifiques de chaque application.\r\n\r\n### Applications pratiques\r\n\r\nLes CNN trouvent aujourd'hui des applications concrètes dans de nombreux secteurs : diagnostic médical par imagerie, systèmes de surveillance automatisée, contrôle qualité industriel, et véhicules autonomes. Leur capacité à traiter efficacement de grandes quantités de données visuelles en fait un outil incontournable pour ces domaines.\r\n\r\n### Perspectives techniques\r\n\r\nL'optimisation continue des architectures CNN, combinée à l'amélioration des capacités de calcul, permet d'envisager des applications plus complexes et une précision accrue dans les tâches de reconnaissance d'images. La compréhension de ces mécanismes fondamentaux reste essentielle pour développer et implémenter efficacement ces solutions technologiques.","src/data/blog/convolutional-neural-networks.mdx","354dbc9032e737c2",true,"global-id",{"id":90,"data":92,"body":100,"filePath":101,"digest":102,"deferredRender":89},{"title":93,"contentType":77,"isDraft":78,"taxonomies":94,"thumbnail":96,"summary":97,"pubDate":98,"author":99},"Global Identifier",[95],{"id":11,"collection":9},"/global_id/GlobalID.png","La génération d'identifiants dans une application est un besoin métier très courant. Que ce soit pour créer des bons de commande, des utilisateurs ou toute autre entité métier, chaque objet a besoin d’un identifiant unique et traçable.",["Date","2025-09-04T00:00:00.000Z"],{"id":64,"collection":62},"La génération d'identifiants dans une application est un besoin métier très courant. Que ce soit pour créer des bons de commande, des utilisateurs ou toute autre entité métier, chaque objet a besoin d’un identifiant **unique et traçable**. Ce besoin devient crucial lorsque l’application commence à **scaler horizontalement** (multiplication des instances, microservices, etc.).\r\n\r\nDans une application classique, on utilise souvent des identifiants auto-incrémentés ou des UUID. Cependant, ces méthodes présentent des limites :\r\n\r\n- Les identifiants incrémentaux sont **hautement prévisibles** (ce qui peut permettre, par exemple, d’estimer le nombre total de commandes ou d’accéder à des ressources non autorisées).\r\n- Les UUID, bien qu’un peu plus sûrs, peuvent être trop longs, peu optimisés pour certaines bases de données, ou encore manquer de **structure métier**.\r\n\r\nUne approche plus structurée et sécurisée devient donc nécessaire pour générer des identifiants **robustes, non prévisibles et compatibles avec une architecture distribuée**.\r\n\r\n## Les limites d’une approche classique\r\n\r\n### Auto-incrémentation\r\n\r\n- Très prévisible : ces identifiants suivent un ordre strict et séquentiel, ce qui les rend facilement devinables. Un attaquant peut ainsi estimer le volume de données (nombre de commandes, d’utilisateurs, etc.), ou tenter des accès non autorisés par simple incrémentation.\r\n\r\n### UUID (notamment UUIDv4)\r\n\r\n- **Poids élevé** : un UUIDv4 occupe 16 octets, contre 4 octets pour un entier auto-incrémenté (INT). Ce surcoût impacte :\r\n  - les performances de la base de données (indexation, tri, recherche)\r\n  - l’utilisation mémoire côté application, surtout à grande échelle\r\n- **Peu lisible et difficile à manipuler** :\r\n  - les UUID sont longs, sans structure lisible et difficiles à interpréter pour un humain.\r\n  - ils compliquent le debug, l’affichage dans les interfaces, ou la construction d’URLs propres.\r\n  - ils ne portent aucune information métier (type d’entité, date de création, tenant, etc.).\r\n- **Risque (théorique) de collision** :\r\n  - bien que rares, les collisions restent possibles si la source d’aléatoire est défaillante.\r\n  - même avec un espace d'identifiants très large (128 bits → 3.4 × 10³⁸ possibilités), une application à très haute volumétrie peut atteindre des seuils où ces risques deviennent réels.\r\n\r\n## Présentation du GID\r\n\r\n### Définition\r\n\r\nUn **Global ID (GID)** est une représentation unique d’une entité dans une application. Il permet d’identifier de manière fiable et traçable n’importe quel objet métier (utilisateur, commande, document, etc.), tout en tenant compte du **contexte du tenant** (par exemple une organisation ou un client dans une application multi-tenant).\r\n\r\n**NB :** un _tenant_ représente une organisation ou un client dans une application _multi-tenant_ (c’est-à-dire une application utilisée par plusieurs organisations).\r\n\r\n### Objectif et justification\r\n\r\nDans cet article, nous allons manipuler un GID de **24 octets**. Ce format n’est pas un standard : la plupart des technologies utilisent plutôt **16 octets**, tel que défini par les normes **ISO/IEC 11578:1996**, **ITU-T Rec. X.667** et le **RFC 4122** (spécification des UUID).\r\n\r\nNous faisons le choix d’un GID sur **24 octets (192 bits)** pour répondre à des besoins spécifiques :\r\n\r\n- garantir une **unicité globale renforcée**,\r\n- inclure directement des **informations métier** (tenant, type d’entité, timestamp, etc.) dans la structure de l’identifiant.\r\n- Optimiser pour le multi-tenant : éviter les jointures coûteuses pour identifier le tenant d'une entité\r\n\r\n**Concernant l'impact performance :** bien que 24 octets soient plus lourds qu'un UUID standard (16 octets), les informations métier intégrées permettent d'éviter de nombreuses jointures SQL, compensant largement ce surcoût. De plus, l'indexation reste efficace grâce à la structure prévisible.\r\n\r\n## Anatomie du GID (explication binaire)\r\n\r\n### Notions de base\r\n\r\n- **Bit** : la plus petite unité de données, représentant une valeur binaire (0 ou 1).\r\n- **Octet** : un regroupement de 8 bits, suffisant pour représenter un caractère ou un élément de données complet.\r\n\r\nEn analogie :\r\n\r\n- un **bit** est comme une lettre élémentaire\r\n- un **octet** est le plus petit “mot” que l’ordinateur peut manipuler directement.\r\n\r\nDonc :\r\n\r\n- 2 octets = 16 bits (2 × 8)\r\n- 8 octets = 64 bits (8 × 8)\r\n- 24 octets = 192 bits (24 × 8)\r\n\r\n**Formule générale :**\r\n\r\n\u003Cimg src=\"/global_id/formule.png\" alt=\"Formule générale\" />\r\n\r\n## Cas pratique\r\n\r\nNous allons implémenter un **GID de 24 octets** avec Go, dans un **système multi-tenant**. Chaque bloc de l’identifiant se voit attribuer une plage précise pour reconstituer les 24 octets\r\n\r\n| Octets | Contenu     | Taille             |\r\n| ------ | ----------- | ------------------ |\r\n| 0-7    | Tenant ID   | 8 octets (64 bits) |\r\n| 8-9    | Entity Type | 2 octets (16 bits) |\r\n| 10-17  | Timestamp   | 8 octets (64 bits) |\r\n| 18-23  | Random      | 6 octets (48 bits) |\r\n\r\nAinsi, le GID contient à la fois des informations métier (tenant, type), un timestamp pour la traçabilité, et des données aléatoires pour garantir l’unicité globale.\r\n\r\n### 1. Déclaration des variables\r\n\r\n```go\r\npackage gid\r\n\r\nimport (\r\n    \"crypto/rand\"\r\n    \"encoding/binary\"\r\n    \"encoding/hex\"\r\n    \"fmt\"\r\n    \"strings\"\r\n    \"time\"\r\n)\r\n\r\nconst (\r\n    GIDSize = 24 // 192 bits au total\r\n)\r\n\r\ntype (\r\n    GID      [GIDSize]byte // GID = tableau de 24 octets\r\n    TenantID [8]byte       // Tenant sur 8 octets\r\n)\r\n\r\n```\r\n\r\n### 2. Initialisation du GID\r\n\r\n```go\r\npackage gid\r\n\r\nfunc NewGID(tenantID TenantID, entityType uint16) {\r\n    var id GID // on initialise un tableau de 24 octets pour notre GID\r\n}\r\n```\r\n\r\n### 3. Séparation des blocs et construction\r\n\r\n```go\r\npackage gid\r\n\r\nfunc NewGID(tenantID TenantID, entityType uint16) (GID, error) {\r\n    var id GID\r\n\r\n    // 1. Tenant ID (octets 0 à 7)\r\n    // On copie les 8 octets de tenantID dans les 8 premiers octets de id\r\n    copy(id[0:8], tenantID[:])\r\n\r\n    // 2. Entity Type (octets 8 à 9)\r\n    // - entityType est un uint16 (16 bits = 2 octets)\r\n    // - On l'encode en bytes Big Endian et on l'écrit dans id[8] et id[9]\r\n    binary.BigEndian.PutUint16(id[8:10], entityType)\r\n\r\n    // 3. Timestamp (octets 10 à 17)\r\n    // - On récupère le timestamp actuel en millisecondes\r\n    // - On le convertit en uint64 (64 bits = 8 octets)\r\n    // - On écrit ces 8 octets dans id[10] à id[17]\r\n    now := time.Now().UnixMilli()\r\n    binary.BigEndian.PutUint64(id[10:18], uint64(now))\r\n\r\n    // 4. Données aléatoires (octets 18 à 23)\r\n    // - Ces 6 octets servent à garantir l'unicité globale du GID\r\n    // - On les remplit avec des bytes aléatoires\r\n    _, err := rand.Read(id[18:24])\r\n    if err != nil {\r\n        return Nil, fmt.Errorf(\"failed to generate random bytes: %v\", err)\r\n    }\r\n\r\n    return id, nil\r\n}\r\n```\r\n\r\n## Avantages du GID\r\n\r\n1. **Traçabilité native** : timestamp intégré pour audit et debug\r\n2. **Context-aware** : tenant et type d'entité directement accessibles\r\n3. **Performances** : évite les jointures pour identifier le contexte\r\n4. **Sécurité** : non-prévisible grâce à la partie aléatoire\r\n5. **Flexibilité** : format structuré mais extensible\r\n\r\n## Conclusion\r\n\r\nLe code présenté ici est spécifique à Go, mais le **concept du GID** peut être transposé dans n’importe quel langage.\r\n\r\nL’objectif principal est de **visualiser comment représenter un identifiant global (GID) de manière informative**, en regroupant des **données métier concrètes** (tenant, type d’entité, timestamp) tout en garantissant :\r\n\r\n- **l’unicité globale**,\r\n- et la **résistance à la prédictibilité**.\r\n\r\nAinsi, le GID permet de créer des identifiants à la fois **robustes, traçables et utiles pour le business**, tout en restant **optimisés pour une architecture distribuée ou multi-tenant**.","src/data/blog/global-id.mdx","c729c1928373d064","histoire-du-design-graphique",{"id":103,"data":105,"body":114,"filePath":115,"digest":116,"deferredRender":89},{"title":106,"contentType":77,"isDraft":78,"taxonomies":107,"thumbnail":110,"summary":111,"pubDate":112,"author":113},"Histoire du design graphique",[108,109],{"id":40,"collection":9},{"id":46,"collection":9},"/new_thumbnail.png","Avant d’être un art de vendre, le design graphique a été un art de rassembler, de dominer, parfois de faire peur.",["Date","2025-07-01T00:00:00.000Z"],{"id":64,"collection":62},"\u003Cblockquote>\r\n  Avant d’être un art de vendre, le design graphique a été un art de rassembler,\r\n  de dominer, parfois de faire peur.\r\n\u003C/blockquote>\r\n\r\nDes boucliers romains aux blasons médiévaux, des drapeaux révolutionnaires aux\r\ninsignes militaires du XXe siècle, chaque symbole, chaque couleur, chaque forme\r\nservait un message : \"Nous sommes unis\", \"Nous sommes puissants\", \"Nous sommes\r\ndifférents\". Ces signes visuels portaient déjà les fonctions que l’on attribue\r\naujourd’hui au design graphique : identité, message, stratégie.\r\n\r\nAujourd’hui, je vous propose un voyage à travers l’histoire du design\r\ngraphique. Véritable socle de toute activité visuelle et de toute expérience commerciale, il sert autant à renforcer l’image de marque d’une entité ou d’une collectivité sociale qu’à orienter notre manière de percevoir, de classer et de juger ce qui nous entoure.\r\n\r\nDans cet article, nous découvrirons quelques mouvements artistiques majeurs\r\nqui ont non seulement façonné l’histoire du design graphique, mais qui\r\ncontinuent d’inspirer les créateurs d’aujourd’hui.\r\n\r\n## Arts and Crafts\r\n\r\n\u003Cimg\r\n  src=\"/images/arts-craft-movement-william-morris-13.jpg\"\r\n  alt=\"Arts & Crafts\"\r\n/>\r\n\r\nLe mouvement Arts & Crafts, né dans les années 1860 au\r\nRoyaume-Uni, a été une réaction directe contre la mécanisation et la production de masse apportées par la Révolution industrielle.\r\n\r\nSon objectif était de réhabiliter le travail artisanal, la\r\nqualité des matériaux, la beauté de l’objet fait main, et l’unité entre\r\nl’artiste, l’artisan et l’objet.\r\n\r\nAujourd'hui, ce courant se reflète dans le minimalisme, le travail fait main\r\net maison, ainsi que dans l'utilisation de matériaux locaux, écologiques et\r\ndurables.\r\n\r\n## Art Nouveau\r\n\r\nL’Art Nouveau est un mouvement artistique né à la fin du XIXe\r\nsiècle, dans la continuité d’Arts & Crafts, et partageant une\r\nmême volonté de rupture avec l’industrialisation et la reproduction académique\r\ndes styles anciens.\r\n\r\nIl se caractérise par l’usage de motifs floraux, formes organiques et lignes courbes, qui traduisent une recherche d’harmonie entre l’homme, la nature et l’art. Ce style visait à intégrer l’art dans le quotidien, en rendant beaux et expressifs les objets les plus utilitaires. Dans l’imaginaire collectif, l’Art Nouveau incarne l’élan de modernité et l’épanouissement d’un homme nouveau, à l’aube du XXe siècle.\r\n\r\n\u003Cimg\r\n  src=\"/images/Tile_panel_flowers_Louvre_OA3919-2-297.jpg\"\r\n  alt=\"Art Nouveau 1\"\r\n/>\r\n\u003Cimg src=\"/images/Art_Nouveau_composition.jpg\" alt=\"Art Nouveau 2\" />\r\n\r\n## Constructivisme russe\r\n\r\n\u003Cimg\r\n  src=\"/images/Alexander-Rodchenko-Books-1924.-Image-via-analogue76.com_.webp\"\r\n  alt=\"Constructivisme russe\"\r\n/>\r\n\r\nLe constructivisme russe est un mouvement artistique né dans\r\nles années 1910-1920 en Russie, dans le contexte tumultueux de la Révolution d’Octobre. Contrairement aux courants précédents, il\r\nrejette toute forme de représentation figurative ou décorative, au profit de formes géométriques strictes, d’une\r\nstructure claire, et d’une esthétique\r\nfonctionnelle.\r\n\r\nCe mouvement considère l’œuvre d’art comme un \"objet construit\", une sorte de squelette en trois dimensions au service de la société. Il prône un art utile, orienté vers l’ industrie, l’architecture et la\r\npropagande et les médias de masse.\r\n\r\n### Héritage contemporain : le design brutaliste sur le web\r\n\r\nL’esthétique du constructivisme russe a profondément\r\ninfluencé ce qu’on appelle aujourd’hui le design brutaliste,\r\nparticulièrement visible sur certains sites web contemporains (mon site).\r\n\r\nCe style brutaliste, tout comme le constructivisme, rejette les conventions esthétiques dominantes, et privilégie des choix radicaux, fonctionnels et sans fioritures :\r\n\r\n- typographies très marquées,\r\n- blocs rigides,\r\n- couleurs franches,\r\n- absence de hiérarchisation \"classique\" de l’information,\r\n- et navigation parfois délibérément déroutante.\r\n\r\nObjectif : choquer, interpeller, s’affranchir du \"web lisse\" pour\r\nproposer une expérience brute et assumée, presque militante.\r\n\r\n## Bauhaus\r\n\r\n\u003Cimg src=\"/images/616zajxgtRL.jpg\" alt=\"Bauhaus\" />\r\n\r\nLe Bauhaus est un mouvement artistique et une école de design\r\nfondée en 1919 à Weimar, en Allemagne, par l’architecte Walter Gropius. Il est souvent considéré comme le socle du design moderne, à la croisée de l’ art, de l’architecture et de l’ industrie.\r\n\r\n### Objectif\r\n\r\nL’idée principale du Bauhaus était de réconcilier l’art et l’artisanat dans une ère dominée par l’industrialisation. Au lieu d’opposer les deux, le Bauhaus cherchait à les unifier pour créer des objets à la fois fonctionnels, esthétiques, et accessibles au plus grand nombre.\r\n\r\n\u003Cblockquote>\r\n  \"La forme suit la fonction\" devient l’un de leurs principes-clés.\r\n\u003C/blockquote>\r\n\r\n### Caractéristiques\r\n\r\n- Formes géométriques simples\r\n- Couleurs primaires\r\n- Absence de fioritures\r\n- Priorité à l’usage plutôt qu’à la décoration\r\n- Conception d’objets du quotidien (meubles, affiches,\r\n  bâtiments)\r\n\r\n### Héritage\r\n\r\nLe Bauhaus a influencé le graphisme, l’architecture, la typographie et même l’urbanisme. Aujourd’hui, on retrouve son esprit dans des interfaces minimalistes, des polices géométriques, des logos simples, ou encore dans les meubles IKEA.\r\n\r\n## Style international (Swiss Style)\r\n\r\n\u003Cimg src=\"/images/Dessau_bauhaus_04.jpg\" alt=\"Style international\" />\r\n\r\nLe Style international, aussi appelé Swiss Style, est un courant graphique né en Suisse dans les années 1950. Il hérite des principes du Bauhaus et se distingue par une approche rigoureuse, fonctionnelle et minimaliste du design.\r\n\r\nCe style repose sur l’usage de grilles, de typographies sans empattement (comme Helvetica) et d’une hiérarchie visuelle claire. L’objectif est de transmettre l’information de manière neutre efficace et lisible, sans ornement superflu.\r\n\r\nCe courant a profondément influencé le design éditorial, l’affiche moderne, la\r\nsignalétique, et plus tard, le webdesign et les interfaces numériques.\r\n\r\n### Éléments clés :\r\n\r\n- Grille de mise en page\r\n- Typographie sans-serif (Helvetica, Univers…)\r\n- Simplicité, clarté, hiérarchie\r\n- Formes géométriques\r\n- Équilibre entre texte et image\r\n\r\n## Pop Art\r\n\r\n\u003Cimg src=\"/images/girl-pop-art-canvas-695502_1600x.webp\" alt=\"Pop Art\" />\r\n\r\nLe Pop Art se reconnaît par ses couleurs vives, ses formes simples et l’usage d’ images populaires comme celles de la pub, des emballages ou des BD.\r\n\r\nIl casse les codes en utilisant des éléments du quotidien pour créer un art\r\naccessible et percutant.\r\n\r\nAujourd’hui, on retrouve son influence dans le design graphique, la publicité, ou encore les réseaux sociaux, où l’image forte et directe est au centre de la communication.\r\n\r\n## Postmodernisme\r\n\r\n\u003Cimg\r\n  src=\"/images/360_F_973913124_ExyhYfl8rIzVqbwKjT7CH3N9fnRzOJUA.jpg\"\r\n  alt=\"Postmodernisme\"\r\n/>\r\n\r\nLe postmodernisme apparaît vers les années 1970, en réaction au modernisme et à ses formes trop strictes, froides et parfois jugées trop sérieuses.\r\n\r\nIl casse les règles : mélange les styles, joue avec les couleurs, reprend le passé tout en y ajoutant une touche d’humour ou de décalage. C’est un courant qui refuse de suivre une seule voie.\r\n\r\nEn architecture, design ou graphisme, le postmodernisme remet en cause l’idée\r\nde “bon goût” et valorise la liberté d’expression, même si\r\ncela choque ou surprend.\r\n\r\nAujourd’hui, on en voit encore les traces dans des créations originales, colorées et inattendues, qui mélangent époques et influences.\r\n\r\n## En résumé\r\n\r\n- Arts & Crafts (1860s) :  \r\n   Retour au fait main, réaction contre l’industrialisation.\r\n\r\n- Art Nouveau (fin XIXe – début XXe) :  \r\n   Lignes courbes, nature, harmonie\r\n\r\n- Constructivisme russe (1910s–1920s) :  \r\n   Formes géométriques, propagande visuelle.\r\n\r\n- Bauhaus (1919–1933) :  \r\n   Fonctionnalité, simplicité, union art et industrie.\r\n\r\n- Style international (1920s–1980s) :  \r\n   Architecture moderne, acier, verre, épure.\r\n\r\n- Style typographique suisse / Style suisse (années 1950) :  \r\n   Grille, typographie claire, Helvetica, influence majeure en graphisme.\r\n\r\n- Pop Art (1950s–1970s) :  \r\n   Couleurs vives, culture de masse, Andy Warhol.\r\n\r\n- Postmodernisme (1970s–1990s) :  \r\n   Rupture avec le modernisme, mélange des styles.\r\n\r\nDu travail artisanal aux créations numériques, le design graphique a sans\r\ncesse évolué pour refléter son époque.  \r\nChaque mouvement raconte une histoire, révèle une vision du monde, et influence\r\nencore aujourd’hui notre manière de communiquer. Comprendre cette histoire nous\r\naide à mieux saisir les défis actuels et à imaginer l’avenir du design, toujours\r\nen quête d’innovation et de sens.","src/data/blog/histoire-du-design-graphique.mdx","6acab002a19748d8","i18n-internationalisation",{"id":117,"data":119,"body":127,"filePath":128,"digest":129,"deferredRender":89},{"title":120,"contentType":77,"isDraft":78,"taxonomies":121,"thumbnail":123,"summary":124,"pubDate":125,"author":126},"Internationalisation : Une Approche Native",[122],{"id":57,"collection":9},"/i18n/Internationalisation.png","Architecture i18n/l10n en React utilisant les standards web natifs.",["Date","2025-12-07T00:00:00.000Z"],{"id":64,"collection":62},"**I18n** et **L10n** sont les abréviations respectives d'**internationalisation** et de **localisation**. Il s'agit d'un workflow conventionnel qui consiste à préparer et adapter une application web au multilinguisme et aux différentes cultures.\r\n\r\nDans cet article, je vous propose une approche architecturale en React basée sur les Contextes, les Hooks, et centrée sur l'API native du navigateur **[Intl](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Intl)**. L'objectif ? Maîtriser le flux de données et comprendre l'approche des frameworks qui proposent des solutions toutes faites, en construisant la nôtre from scratch.\r\n\r\n## La structure de l'API\r\n\r\nL'approche repose sur trois piliers fondamentaux :\r\n\r\n1- **Global State Management** : Un `TranslatorProvider` qui entoure l'application et distribue via le Context API la locale active, le dictionnaire de traductions chargé dynamiquement, et la fonction de traduction.\r\n\r\nLe hook `useTranslator` expose ensuite une API unifiée pour accéder aux traductions (`__`), aux formats de dates (`dateFormat`) et aux formats numériques (`numberFormat`).\r\n\r\n2- **Standardisation Native** : Déléguer la complexité du formatage (dates, nombres, pluriels) à l'API **`Intl`** du navigateur.\r\n\r\n3- **Outillage Statique** : Un script d'analyse statique qui extrait automatiquement les clés de traduction du code source via regex, puis synchronise les fichiers JSON de chaque locale en ajoutant les nouvelles clés et supprimant celles devenues obsolètes. Cela garantit que les dictionnaires restent toujours cohérents avec le code.\r\n\r\n## L'Architecture de Données\r\n\r\nMon approche dans la structuration des données s'est naturellement tournée vers JSON. Étant donné le nombre de méthodes natives du navigateur qui permettent un parsing de ce format (`JSON.parse`, `fetch`, etc.), j'ai choisi de m'appuyer sur cette simplicité.\r\n\r\n```json\r\n{\r\n  \"Bonjour\": \"Hello\",\r\n  \"Au revoir\": \"Goodbye\"\r\n}\r\n```\r\n\r\n### State Management (State Sharing)\r\n\r\nL'architecture s'appuie sur le pattern **Provider/Consumer** de React. Le `TranslatorProvider` met le dictionnaire à disposition du Context, et les composants le consomment via le hook `useTranslator`.\r\n\r\n```typescript\r\n// TranslatorProvider.tsx\r\nexport function TranslatorProvider({ children, lang, loader }: Props) {\r\n  const [langV, setLangV] = useState\u003CLangs>(lang);\r\n  const [translations, setTranslations] = useState\u003CRecord\u003Cstring, string>>({});\r\n\r\n  useEffect(() => {\r\n    loader(langV).then(setTranslations);\r\n  }, [langV, loader]);\r\n\r\n  const translate = useCallback(\r\n    (s: string) => translations[s] || s,\r\n    [translations]\r\n  );\r\n\r\n  return (\r\n    \u003CTranslateContext.Provider value={{ translate, langV, setLangV }}>\r\n      {children}\r\n    \u003C/TranslateContext.Provider>\r\n  );\r\n}\r\n```\r\n\r\n```typescript\r\nexport function useTranslator() {\r\n  const { translate, langV, setLangV } = useContext(TranslateContext);\r\n\r\n  const dateFormat = (date: Date | string | number | null,\r\n    options: Intl.DateTimeFormatOptions = {\r\n      year: \"numeric\",\r\n      month: \"long\",\r\n      day: \"numeric\",\r\n      weekday: \"long\",\r\n    }\r\n  ) => {...};\r\n\r\n  const styleForNumber = (options: Options): Intl.NumberFormatOptions ...\r\n\r\n  const numberFormat = ( value: number | string | null, options: Options = {\r\n      style: \"decimal\",\r\n    }\r\n  ) => {...};\r\n\r\n  return {\r\n    langV,\r\n    setLangV,\r\n    dateFormat: dateFormat,\r\n    numberFormat,\r\n    __: translate,\r\n  };\r\n}\r\n```\r\n\r\n**Pourquoi cette approche ?**\r\nElle sépare le chargement `loader` de la distribution `Provider`. Cela rend le système agnostique : les traductions peuvent venir d'un fichier JSON local, d'une API REST ou d'un CMS headless.\r\n\r\n## Abstraction autour d'Intl\r\n\r\nL'API Intl du navigateur couvre la plupart des besoins d'internationalisation pour une application web. Elle contient l'intégralité des règles linguistiques et typographiques mondiales : formats de dates par pays, symboles monétaires, règles de pluralisation, etc.\r\n\r\nUne **façade légère** suffit pour adapter cette API à nos besoins. L'idée est d'encapsuler la verbosité d'`Intl` (constructeurs, options multiples) derrière des fonctions simples (`dateFormat`, `numberFormat`) qui respectent automatiquement la locale active.\r\n\r\n```typescript\r\n// useTranslator.ts\r\nconst styleForNumber = (options: Options): Intl.NumberFormatOptions => {\r\n  switch (options.style) {\r\n    case \"currency\":\r\n      return {\r\n        style: \"currency\",\r\n        // La devise est déduite dynamiquement de la locale (ex: FR -> EUR)\r\n        currency: CurrencyCode[langV.toUpperCase()],\r\n      };\r\n    case \"unit\":\r\n      return {\r\n        style: \"unit\",\r\n        unit: options.unit, // ex: \"kilometer\"\r\n      };\r\n    // ...\r\n  }\r\n};\r\n```\r\n\r\nCette fonction agit comme un **normalisateur** qui garantit le respect des conventions typographiques selon la locale active : espace insécable, position du symbole monétaire, séparateur décimal (virgule ou point).\r\n\r\n## Automatisation de la Maintenance\r\n\r\nPour des raisons de maintenance et d'expérience développeur, j'ai implémenté un script qui analyse le code source et synchronise automatiquement les fichiers de traduction.\r\n\r\nL'outil scanne tous les fichiers TypeScript/React pour détecter les appels à `__()` via regex, puis met à jour les dictionnaires de chaque locale.\r\n\r\n```typescript\r\n// scripts/collect.ts\r\nfunction collectStrings() {\r\n  const files = globSync(sourcePath);\r\n  const strings = new Set\u003Cstring>();\r\n\r\n  for (const file of files) {\r\n    const content = readFileSync(file, \"utf8\");\r\n    const found = extractStringsFromContent(content);\r\n    found.forEach((s) => strings.add(s));\r\n  }\r\n  return strings;\r\n}\r\n\r\n// Synchronisation : ajout des nouvelles clés, suppression des obsolètes\r\nfor (const newKey of keys.difference(existingKeys)) {\r\n  dict.set(newKey, \"\");\r\n}\r\n```\r\n\r\nUne simple commande (`node scripts/collect.ts` ou via votre task runner) garantit que les fichiers JSON restent parfaitement alignés avec le code.\r\n\r\n## Application Concrète\r\n\r\nL'interface ci-dessous réagit au changement de locale :\r\n\r\n- Formats de date adaptés\r\n- Devises localisées (€ vs $)\r\n- Unités régionales (km, °C)\r\n\r\n\u003Ciframe\r\n  src=\"https://elyseemb.github.io/i18Tp/\"\r\n  width=\"100%\"\r\n  height=\"800px\"\r\n  style=\"border-radius: 18px;\"\r\n  title=\"Démonstration du système i18n natif\"\r\n  loading=\"lazy\"\r\n>\u003C/iframe>\r\n\r\n## Conclusion\r\n\r\nCette architecture illustre que les navigateurs modernes fournissent déjà de nombreuses abstractions puissantes. Ces standards sont optimisés, maintenus par les éditeurs de navigateurs.\r\n\r\nL'implémentation complète (Provider, Hook, script d'extraction) est disponible sur [GitHub](https://github.com/elyseeMB/i18Tp) pour expérimentation et adaptation à vos projets.","src/data/blog/i18n-(internationalisation).mdx","571072a548d74175","lazy-loading",{"id":130,"data":132,"body":140,"filePath":141,"digest":142,"deferredRender":89},{"title":133,"contentType":77,"isDraft":78,"taxonomies":134,"thumbnail":136,"summary":137,"pubDate":138,"author":139},"Lazy Loading React : Retry & Reload",[135],{"id":57,"collection":9},"/lazy_loading/lazy_loading.png","En production, dans une application SPA (Single Page Application), il est crucial d'optimiser le chargement et le déploiement de nouvelles versions.",["Date","2025-09-14T00:00:00.000Z"],{"id":64,"collection":62},"En production, dans une application SPA (Single Page Application), il est crucial d'optimiser le chargement et le déploiement de nouvelles versions. Il est très fréquent qu'un utilisateur ayant une session active depuis plusieurs heures se retrouve avec une version obsolète de l'application au moment d'un déploiement. Les fichiers JavaScript restent alors en cache dans le navigateur ou actifs durant la session, provoquant des erreurs de chargement lorsque l'utilisateur tente d'accéder à de nouvelles fonctionnalités.\r\n\r\nDans cet article, nous analyserons l'implémentation du lazy loading dans un contexte de navigation et de routage. Nous verrons comment cette approche améliore les performances tout en gérant efficacement les erreurs de chargement en production.\r\n\r\n## Définition\r\n\r\nLe lazy loading est une technique d'optimisation utilisée en programmation, particulièrement en développement web, qui vise à améliorer les performances et réduire les temps de chargement initiaux. Cette approche consiste à différer le chargement des ressources (images, modules JavaScript, composants, etc.) jusqu'à ce qu'elles soient réellement nécessaires ou demandées par l'utilisateur.\r\n\r\n## Le lazy loading React\r\n\r\n### Le Standard\r\n\r\nLa plupart des frameworks modernes (React, Preact, etc.) proposent une implémentation native du lazy loading. React expose cette fonctionnalité via `React.lazy()`, mais cette approche basique révèle ses limites en production, particulièrement lors d'échecs de chargement des chunks JavaScript.\r\n\r\n### Fonctionnement\r\n\r\nLe lazy fonctionne en attendant qu'une promise se résolve vers un module qui contient un export default. Lorsque le composant est demandé React extrait le `module.default` et l'utilise pour le rendu\r\n\r\n### Code d'exemple basique\r\n\r\n```js\r\nimport { lazy } from \"react\";\r\nconst MarkdownPreview = lazy(() => import(\"./MarkdownPreview.js\"));\r\n```\r\n\r\n## Problème de chunks en production\r\n\r\nComme mentionné en introduction, les déploiements fréquents en production génèrent un problème récurrent : les utilisateurs ayant une session active se retrouvent avec des références vers d'anciens chunks qui n'existent plus sur le serveur. Lorsque ces utilisateurs tentent de naviguer vers une nouvelle section de l'application, le chargement échoue car les fichiers JavaScript ont été renommés avec de nouveaux hashs lors du déploiement.\r\n\r\n### Définition d'un chunk :\r\n\r\nUn chunk est un morceau de code JavaScript généré et optimisé pour la production par un bundler (Vite, Webpack) lors du processus de build.\r\n\r\n## Approche améliorée\r\n\r\n### Fonctionnalité Reload\r\n\r\nCommençons par l'essentiel : la gestion des chunks pour minimiser les échecs de chargement dus aux hashs obsolètes des anciennes versions.\r\n\r\n```js\r\nimport { lazy as reactLazy, type ComponentType, type FC } from \"react\";\r\n\r\nexport function createLazy(\r\n  importFunction: () => Promise\u003C{ default: ComponentType\u003Cany> }>,\r\n  maxRetries = 3\r\n) {\r\n  /**\r\n   * 1 - Gestion du reload automatique\r\n   */\r\n  return reactLazy(async () => {\r\n    /**\r\n     * On utilise la signature de la fonction comme clé pour le sessionStorage.\r\n     * On la transforme en chaîne de caractères pour créer un identifiant unique.\r\n     */\r\n    const functionString = importFunction.toString();\r\n\r\n    try {\r\n      /**\r\n       * Ici on exécute la fonction qui importe notre composant\r\n       */\r\n      const component = await importFunction();\r\n\r\n      /**\r\n       * En cas de succès, on supprime au préalable la clé du sessionStorage\r\n       * pour une réinitialisation propre lors du prochain échec éventuel\r\n       */\r\n      sessionStorage.removeItem(functionString);\r\n\r\n      return component;\r\n    } catch (error) {\r\n      /**\r\n       * On récupère la valeur stockée dans le sessionStorage avec la clé \"functionString\".\r\n       * Ensuite on la compare à notre paramètre maxRetries.\r\n       * Si elle est inférieure à maxRetries, on l'incrémente jusqu'à ce qu'elle devienne supérieure.\r\n       * Puis on rafraîchit la page avec window.location.reload().\r\n       */\r\n      const currentFailures = parseInt(\r\n        sessionStorage.getItem(functionString) || \"0\"\r\n      );\r\n\r\n      if (currentFailures \u003C maxRetries) {\r\n        sessionStorage.setItem(\r\n          functionString,\r\n          (currentFailures + 1).toString()\r\n        );\r\n        window.location.reload();\r\n\r\n        /**\r\n         * React.lazy() attend une Promise qui se résout vers { default: Component }.\r\n         * Ici c'est un placeholder qui ne sera jamais affiché car la page se recharge.\r\n         * On retourne une signature similaire à l'API React.lazy().\r\n         */\r\n        const EmptyComponent: FC = () => null;\r\n        return { default: EmptyComponent };\r\n      }\r\n\r\n      /**\r\n       * Si on a dépassé le nombre maximum de tentatives, on propage l'erreur\r\n       */\r\n      throw error;\r\n    }\r\n  });\r\n}\r\n```\r\n\r\nCette approche nous permet d'assurer une gestion automatique des erreurs de chunks et d'améliorer significativement l'expérience utilisateur en évitant les crashs d'application.\r\n\r\n### Logique du retry : Optimisation\r\n\r\nEnsuite, dans le but d'assurer une expérience utilisateur optimale, nous ajoutons un paramètre retry qui définit le nombre de tentatives avant de déclencher le reload. Ainsi, nous nous assurons que les erreurs temporaires (connexion instable, serveur momentanément indisponible) sont résolues sans avoir recours au rechargement de page, qui reste notre solution de dernier recours pour les chunks réellement obsolètes.\r\n\r\n```js\r\nconst tryImport = async (): Promise\u003C{ default: ComponentType\u003Cany> }> => {\r\n  /**\r\n   * On initialise le compteur\r\n   */\r\n  let retryCount: number = 0;\r\n\r\n  /**\r\n   * Fonction interne pour gérer le retry d'import.\r\n   * On encapsule attempt pour que retryCount soit local à chaque appel\r\n   * et éviter de partager l'état entre plusieurs imports.\r\n   * Ainsi, on a une fonction locale qui gère son propre état et ses manipulations.\r\n   */\r\n  const attempt = async (): Promise\u003C{ default: ComponentType\u003Cany> }> => {\r\n    try {\r\n      return await importFunction();\r\n    } catch (error) {\r\n      /**\r\n       * Si le nombre de reprises est inférieur à l'argument mis en paramètre (qui était de 3),\r\n       * on incrémente et réexécute la fonction (récursivité)\r\n       */\r\n      if (retryCount \u003C importRetries) {\r\n        retryCount++;\r\n        /**\r\n         * On marque un temps d'arrêt pour chaque reprise\r\n         */\r\n        if (retryDelay > 0) {\r\n          await new Promise((resolve) => setTimeout(resolve, retryDelay));\r\n        }\r\n\r\n        return attempt();\r\n      }\r\n      /**\r\n       * En cas d'échec répété, on arrête et on renvoie l'erreur\r\n       */\r\n      throw error;\r\n    }\r\n  };\r\n\r\n  return attempt();\r\n};\r\n```\r\n\r\nEncapsuler `attempt` permet de garder le compteur de retry local à chaque tentative de chargement, évitant les conflits ou réinitialisations inattendues dans un environnement React où le lazy loading peut être évalué plusieurs fois.\r\n\r\n### Le résultat :\r\n\r\n```js\r\nimport { lazy as reactLazy, type ComponentType, type FC } from \"react\";\r\n\r\ntype Options = {\r\n  maxRetries: number,\r\n  importRetries: number,\r\n  retryDelay: number,\r\n};\r\n\r\nexport function createLazy(\r\n  importFunction: () => Promise\u003C{ default: ComponentType\u003Cany> }>,\r\n  { maxRetries, importRetries, retryDelay }: Options = {\r\n    maxRetries: 3,\r\n    importRetries: 3,\r\n    retryDelay: 300,\r\n  }\r\n) {\r\n  /**\r\n   * Retry feature\r\n   *\r\n   * On crée une fonction avec une signature similaire à ce que doit retourner createLazy()\r\n   */\r\n  const tryImport = async (): Promise\u003C{ default: ComponentType\u003Cany> }> => {\r\n    /**\r\n     * On initialise le compteur\r\n     */\r\n    let retryCount: number = 0;\r\n\r\n    /**\r\n     * Fonction interne pour gérer le retry d'import.\r\n     * On encapsule attempt pour que retryCount soit local à chaque appel\r\n     * et éviter de partager l'état entre plusieurs imports.\r\n     * Ainsi, on a une fonction locale qui gère son propre état et ses manipulations.\r\n     */\r\n    const attempt = async (): Promise\u003C{ default: ComponentType\u003Cany> }> => {\r\n      try {\r\n        return await importFunction();\r\n      } catch (error) {\r\n        /**\r\n         * Si le nombre de reprises est inférieur à l'argument mis en paramètre (qui était de 3),\r\n         * on incrémente et réexécute la fonction (récursivité)\r\n         */\r\n        if (retryCount \u003C importRetries) {\r\n          retryCount++;\r\n          /**\r\n           * On marque un temps d'arrêt pour chaque reprise\r\n           */\r\n          if (retryDelay > 0) {\r\n            await new Promise((resolve) => setTimeout(resolve, retryDelay));\r\n          }\r\n\r\n          return attempt();\r\n        }\r\n        /**\r\n         * En cas d'échec répété, on arrête et on renvoie l'erreur\r\n         */\r\n        throw error;\r\n      }\r\n    };\r\n\r\n    return attempt();\r\n  };\r\n\r\n  /**\r\n   * Gestion du reload automatique\r\n   */\r\n  return reactLazy(async () => {\r\n    /**\r\n     * On utilise la signature de la fonction comme clé pour le sessionStorage.\r\n     * On la transforme en chaîne de caractères pour créer un identifiant unique.\r\n     */\r\n    const functionString = importFunction.toString();\r\n\r\n    try {\r\n      /**\r\n       * Ici on exécute la fonction tryImport qui importe notre composant\r\n       * et fait des traitements au préalable avant de faire un reload\r\n       */\r\n      const component = await tryImport();\r\n\r\n      /**\r\n       * En cas de succès, on supprime au préalable la clé du sessionStorage\r\n       * pour une réinitialisation propre lors du prochain échec éventuel\r\n       */\r\n      sessionStorage.removeItem(functionString);\r\n\r\n      return component;\r\n    } catch (error) {\r\n      /**\r\n       * On récupère la valeur stockée dans le sessionStorage avec la clé \"functionString\".\r\n       * Ensuite on la compare à notre paramètre maxRetries.\r\n       * Si elle est inférieure à maxRetries, on l'incrémente jusqu'à ce qu'elle devienne supérieure.\r\n       * Puis on rafraîchit la page avec window.location.reload().\r\n       */\r\n      const currentFailures = parseInt(\r\n        sessionStorage.getItem(functionString) || \"0\"\r\n      );\r\n\r\n      if (currentFailures \u003C maxRetries) {\r\n        sessionStorage.setItem(\r\n          functionString,\r\n          (currentFailures + 1).toString()\r\n        );\r\n\r\n        window.location.reload();\r\n\r\n        /**\r\n         * React.lazy() attend une Promise qui se résout vers { default: Component }.\r\n         * Ici c'est un placeholder qui ne sera jamais affiché car la page se recharge.\r\n         * On retourne une signature similaire à l'API React.lazy().\r\n         */\r\n        const EmptyComponent: FC = () => null;\r\n\r\n        return { default: EmptyComponent };\r\n      }\r\n      /**\r\n       * Si on a dépassé le nombre maximum de tentatives, on propage l'erreur\r\n       */\r\n      throw error;\r\n    }\r\n  });\r\n}\r\n```\r\n\r\n## Solutions apportées par cette approche\r\n\r\nCette implémentation de `createLazy` apporte plusieurs améliorations significatives par rapport au lazy loading standard :\r\n\r\n1. **Gestion robuste des erreurs et retries**  \r\n   Le composant est importé avec un système de retry configurable (`importRetries`) et un délai entre les tentatives (`retryDelay`). Cela permet de mieux gérer les problèmes de réseau ou les échecs temporaires lors du chargement des composants, réduisant ainsi les risques de plantage de l'application.\r\n2. **Gestion efficace des chunks**  \r\n   En contrôlant le lazy loading au niveau des imports et en utilisant des clés uniques pour chaque fonction importée, cette approche facilite la gestion des chunks JavaScript pour la production. Cela optimise la livraison des ressources et simplifie le **déploiement continu**, car les composants problématiques peuvent être rechargés automatiquement sans interrompre l'expérience utilisateur.\r\n3. **Potentiel d'amélioration de l'expérience utilisateur**  \r\n   Bien que cette version se concentre sur la robustesse et la fiabilité, il est possible d'étendre le mécanisme pour inclure le suivi de l'état de chargement. Par exemple : afficher un **squelette** ou un indicateur de progression pendant que le composant est en cours de chargement. Cela améliorerait directement l'expérience utilisateur tout en conservant les bénéfices du retry et du reload automatique.\r\n\r\n## Simulation\r\n\r\nPour démontrer le fonctionnement de mon code, j'ai volontairement fait échouer certains composants en lazy loading. Chaque composant tente de se recharger plusieurs fois avec un délai entre chaque essai, et si ça échoue encore, la page peut se recharger automatiquement. L'objectif n'est pas d'avoir une interface fluide, mais de montrer concrètement comment le mécanisme de retry et de reload fonctionne.\r\n\r\n\u003Ciframe\r\n  src=\"https://elyseemb.github.io/lazyLoading/\"\r\n  style=\"width:100%; height:500px; border:0; border-radius:8px;\"\r\n>\u003C/iframe>\r\n\r\nLe code GitHub : [https://github.com/elyseeMB/lazyLoading.git](https://github.com/elyseeMB/lazyLoading.git)\r\n\r\n## Conclusion\r\n\r\nEn résumé, cette approche permet d'avoir un lazy loading plus fiable et résilient, avec des bénéfices concrets pour la production et la maintenance. Elle résout efficacement le problème critique des chunks obsolètes qui affecte quotidiennement les utilisateurs d'applications SPA.\r\n\r\nCette implémentation constitue une base solide adaptable selon vos besoins : monitoring, feedback utilisateur, configuration par environnement. L'avantage principal réside dans sa simplicité d'adoption - quelques lignes suffisent pour transformer un lazy loading fragile en système robuste.","src/data/blog/lazy-loading.mdx","0f9b684e9d337c33","load-balancer",{"id":143,"data":145,"body":153,"filePath":154,"digest":155,"deferredRender":89},{"title":146,"contentType":77,"isDraft":78,"taxonomies":147,"thumbnail":149,"summary":150,"pubDate":151,"author":152},"Le Load Balancer",[148],{"id":35,"collection":9},"/load_balancer/load_balancer.png","Le load balancing, ou équilibrage de charge, est une technique utilisée pour améliorer la disponibilité, la scalabilité et la résilience des applications web modernes en répartissant le trafic entre plusieurs serveurs.",["Date","2025-07-21T00:00:00.000Z"],{"id":64,"collection":62},"Chaque jour, des milliers d’utilisateurs se connectent pour consulter leurs\r\ne-mails, échanger des messages ou travailler sur leurs projets. Un nombre\r\nincalculable de paquets réseau transitent chaque seconde entre des milliers de\r\nserveurs, tous optimisés pour répondre le plus rapidement possible et\r\ndistribuer efficacement les ressources.\r\n\r\nMais qui dit **multiplication des utilisateurs**, dit aussi **requêtes\r\nsimultanées**, risques de surcharge, lenteurs, voire pannes.\r\n\r\nDans cette course à la performance et à la réactivité, de nombreuses techniques\r\nsont mises en œuvre pour garantir une distribution fluide et équilibrée de la\r\ncharge : **le load balancing** en est l’une des clés.\r\n\r\n**Le Load Balancing** est un processus clé en informatique, souvent utilisé\r\ndans l’architecture des systèmes distribués. Il permet de répartir un ensemble\r\nde tâches ou de requêtes sur plusieurs ressources (comme des serveurs) afin de\r\n**réduire la surcharge**, **améliorer les performances globales** et\r\n**garantir une haute disponibilité**. Il s’applique principalement **au niveau\r\ndes protocoles d’application** tels que **HTTP/HTTPS, FTP, SMTP, DNS, SSH**,\r\netc., pour gérer efficacement le trafic réseau.\r\n\r\n\u003Cimg\r\n  src=\"/load_balancer/illustration_load-balancer.png\"\r\n  alt=\"Schéma d'un load balancer distribuant le trafic\"\r\n/>\r\n\r\nConcrètement, au lieu que chaque client s’adresse directement à un serveur\r\ndonné, toutes les requêtes sont envoyées à une **adresse réseau centrale** —\r\ncelle du load balancer. Celui-ci se charge alors de **rediriger le trafic**\r\nvers les serveurs disponibles, selon différents critères (algorithmes, charge\r\nactuelle, disponibilité, etc.).\r\n\r\nCe mécanisme permet d’éviter les surcharges, d’améliorer les performances globales du système et de garantir une haute disponibilité du service.\r\n\r\n## Load Balancing : niveau 4 vs niveau 7\r\n\r\nIl existe plusieurs types de load balancers, selon le niveau du **modèle OSI** sur\r\nlequel ils opèrent :\r\n\r\n- **Niveau 4 (transport)** : l’équilibrage se fait sur la base des adresses IP, des ports TCP ou UDP. Le load balancer ne « voit » pas le contenu des requêtes, il se contente de router le trafic selon des règles de bas niveau.  \r\n   Exemple : **HAProxy** ou **AWS Network Load Balancer**.\r\n- **Niveau 7 (application)** : l’équilibrage se fait en analysant le contenu des requêtes HTTP, comme l’URL, les cookies, ou les en-têtes. Cela permet une répartition plus fine et contextuelle.  \r\n   Exemple : **NGINX**, **Traefik**, ou **AWS Application Load Balancer**.\r\n\r\nCe choix dépend du type d’application, du besoin en personnalisation et des performances attendues.\r\n\r\n## Principaux algorithmes d’équilibrage:\r\n\r\n### Le Round Robin DNS :\r\n\r\n\u003Cimg src=\"/load_balancer/round_robin.png\" alt=\"round_rogin\" />\r\n\r\nLe **Round Robin DNS** est un algorithme d’équilibrage de charge qui permet de\r\nrépartir le trafic entre plusieurs serveurs en associant **plusieurs adresses\r\nIP** à un **même nom de domaine**.\r\n\r\nContrairement à d'autres méthodes, cette technique **ne nécessite aucun\r\néquipement physique** dédié. Elle repose sur le fonctionnement du **serveur\r\nDNS autoritaire** ([authoritative\r\nnameserver](https://www.cloudflare.com/learning/dns/dns-server-types/#authoritative-nameserver)).\r\n\r\nElle est facile à mettre en place via l’interface de gestion DNS de votre fournisseur :\r\n\r\n- en ajoutant **plusieurs enregistrements A** (pour IPv4),\r\n- ou des **enregistrements AAAA** (pour IPv6).\r\n\r\n**Avantage** : simplicité de configuration.  \r\n**Limite** : pas de gestion intelligente de la charge réelle (le DNS ne “voit” pas si un serveur est saturé ou indisponible).\r\n\r\n## load balancers logiciels et matériels:\r\n\r\n## -- logicielles:\r\n\r\n### [Nginx](https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/) :\r\n\r\nnginx est un logiciel open-source qui fait office de **serveur web**, de\r\n**reverse proxy** et de **load balancer**. Il est reconnu pour sa **faible\r\nconsommation mémoire** et sa **grande rapidité**, ce qui en fait un choix\r\nprivilégié dans les environnements à fort trafic.\r\n\r\nPour configurer Nginx comme load balancer, on utilise la directive `upstream` dans le fichier de configuration, qui permet de déclarer plusieurs serveurs backend :\r\n\r\n```nginx\r\nhttp {\r\n    upstream backend_servers {\r\n        server backend1.example.com;\r\n        server backend2.example.com;\r\n        server backend3.example.com;\r\n    }\r\n\r\n    server {\r\n        listen 80;\r\n\r\n        location / {\r\n            proxy_pass http://backend_servers;\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nPar défaut, Nginx applique le **round robin**, mais il supporte aussi :\r\n\r\n- `least_conn` : vers le serveur avec le moins de connexions actives\r\n- `ip_hash` : pour garder la session sur le même serveur\r\n\r\n## choisir une méthode d’équilibrage de charge:\r\n\r\n[NGINX](https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/#choosing-a-load-balancing-method \"Choosing a Load Balancing Method\") en charge quatre méthodes d'équilibrage de charge : Round Robin, Least Connections, IP Hash et Generic Hash.\r\n\r\n> **Note:** Lors de la configuration d'une méthode autre que Round Robin, mettez la directive correspondante ([`hash`](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#hash), [`ip_hash`](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#ip_hash), [`least_conn`](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#least_conn), [`least_time`](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#least_time), ou [`random`](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#random)) au-dessus de la liste de `server` directives dans le [`upstream {}`](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#upstream) bloc.\r\n\r\n1- **Round Robin (Default)**\r\n\r\n```nginx\r\nupstream backend {\r\n   # no load balancing method is specified for Round Robin\r\n   server backend1.example.com;\r\n   server backend2.example.com;\r\n}\r\n```\r\n\r\n    Contrairement au Round Robin DNS, les reverse proxies comme **NGINX** permettent un équilibrage de charge **plus intelligent** et dynamique, avec prise en compte des connexions actives, de l’adresse IP du client, ou de la clé de hachage.\r\n\r\n2- **[Least\r\nConnections](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#least_conn) –**\r\n Une requête est envoyée au serveur avec le moins de connexions actives. Cette\r\nméthode prend également [poids du\r\nserveur](https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/#weights) en\r\nconsidération.\r\n\r\n```nginx\r\nupstream backend {\r\n    least_conn;\r\n    server backend1.example.com;\r\n    server backend2.example.com;\r\n}\r\n```\r\n\r\n3- **[IP\r\nHash](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#ip_hash) –**\r\nLe serveur auquel une requête est envoyée est déterminé à partir de l'adresse\r\nIP du client. Dans ce cas, soit les trois premiers octets de l'adresse IPv4,\r\nsoit l'adresse IPv6 entière sont utilisés pour calculer la valeur de hachage.\r\nLa méthode garantit que les requêtes provenant de la même adresse parviennent\r\nau même serveur, sauf si celui-ci n'est pas disponible.\r\n\r\n```nginx\r\nupstream backend {\r\n    ip_hash;\r\n    server backend1.example.com;\r\n    server backend2.example.com;\r\n}\r\n```\r\n\r\nSi l'un des serveurs doit être temporairement retiré de la rotation de\r\nchargement‑équilibrage, il peut être marqué avec\r\nle [down](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#down) paramètre.\r\nCela préserve le hachage actuel des adresses IP des clients. Les requêtes qui\r\ndevaient être traitées par ce serveur sont automatiquement envoyées au serveur\r\nsuivant du groupe.\r\n\r\n```nginx\r\nupstream backend {\r\n    server backend1.example.com;\r\n    server backend2.example.com;\r\n    server backend3.example.com down;\r\n}\r\n```\r\n\r\n4- **Generic [Hash](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#hash) –**\r\nLe serveur auquel une requête est envoyée est déterminé à partir d'une clé\r\ndéfinie par l'utilisateur‑. Cette clé peut être une chaîne de texte, une\r\nvariable ou une combinaison. Par exemple, la clé peut être une adresse IP\r\nsource et un port appariés. Cet exemple utilise un URI :\r\n\r\n```nginx\r\nupstream backend {\r\n    hash $request_uri consistent;\r\n    server backend1.example.com;\r\n    server backend2.example.com;\r\n}\r\n```\r\n\r\n    L'optionnel [cohérent](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#hash) paramètre au `hash` la directive permet [ketama](http://www.last.fm/user/RJ/journal/2007/04/10/rz_libketama_-_a_consistent_hashing_algo_for_memcache_clients) cohérent‑équilibrage de charge de hachage. Les requêtes sont réparties uniformément sur tous les serveurs en amont en fonction de la valeur de clé hachée définie par l'utilisateur‑. Si un serveur en amont est ajouté ou supprimé d'un groupe en amont, seules quelques clés sont remappées, ce qui minimise les échecs de cache. Ceci est utile pour équilibrer la charge des serveurs de cache ou d’autres applications qui accumulent de l’état.\r\n\r\n### [HAProxy](https://www.haproxy.com/blog/haproxy-configuration-basics-load-balance-your-servers):\r\n\r\nHAProxy est un logiciel gratuit et open source qui offre une haute disponibilité et un équilibrage de charge pour les applications basées sur TCP et HTTP. Il répartit le trafic réseau entrant sur plusieurs serveurs pour garantir une utilisation et une évolutivité optimales.\r\n\r\n**HAProxy** et **NGINX** peuvent tous deux faire de l’équilibrage de charge,\r\nmais ils ont des **différences importantes** dans leur conception, leur\r\ncomportement et leurs cas d’usage préférés.\r\n\r\n## NGINX vs HAProxy : Comparaison d’usage\r\n\r\n\u003Cdiv class=\"table-container\">\r\n\r\n| Critère                      | **NGINX**                           | **HAProxy**                              |\r\n| ---------------------------- | ----------------------------------- | ---------------------------------------- |\r\n| **Fonction principale**      | Serveur HTTP + reverse proxy        | Load balancer (spécialisé)               |\r\n| **Performance brute**        | Excellente en HTTP, bon généraliste | Meilleure sur les charges réseau élevées |\r\n| **Équilibrage niveau**       | L7 (HTTP) + partiel L4              | L4 (TCP) + L7 (HTTP) très optimisé       |\r\n| **Support HTTPS natif**      | Oui (certbot, etc.)                 | Possible, mais plus complexe             |\r\n| **Configuration**            | Simple, fichiers de config lisibles | Plus verbeux mais très précis            |\r\n| **Monitoring / stats**       | Basique (module de status)          | Très détaillé (dashboard intégré)        |\r\n| **Utilisation fréquente**    | Reverse proxy web, CDN, cache       | Load balancing pur, haute disponibilité  |\r\n| **Consommation mémoire**     | Faible                              | Ultra-optimisée aussi                    |\r\n| **Hot reload / live update** | Pas toujours sans coupure           | Oui, sans perturber les connexions       |\r\n\r\n\u003C/div>\r\n\r\n## Load Balancer matériel\r\n\r\n    En support physique, En version matérielle, ce sont des dispositifs physiques installés dans des datacenters spécifiques. Bien qu'il soient capable de gérer et dispatcher un grand volume de trafic sur différent réseau, Ils offrent moins de flexibilité et leurs coûts sont assez élevés.\r\n\r\n\u003Cdiv class=\"table-container\">\r\n\r\n| Nom                             | Description                                                                                                                            |\r\n| ------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |\r\n| **F5 BIG-IP**                   | Le plus connu. Très utilisé en entreprise. Permet du L4 et L7 avec des fonctions avancées (SSL offloading, firewall applicatif, etc.). |\r\n| **Cisco ACE / ACI**             | Intégré dans les solutions réseau Cisco. Moins courant aujourd’hui, mais très robuste dans certains data centers.                      |\r\n| **Barracuda Load Balancer ADC** | Connu pour sa simplicité, bon rapport qualité/prix, adapté aux PME. Propose aussi des fonctions de sécurité.                           |\r\n\r\n\u003C/div>\r\n\r\n---\r\n\r\n> Dans cet article, nous avons clarifié et expliqué quelques-uns des algorithmes de **load balancing** les plus utilisés.  \r\n> Mais cette liste est loin d’être exhaustive : des outils comme **HAProxy** proposent d'autres méthodes avancées de répartition, ainsi que des fonctionnalités clés telles que :\r\n>\r\n> - les **health checks** (vérification automatique de l’état des serveurs),\r\n> - la **redondance** avec des load balancers en mode **actif/passif**,\r\n> - la **reprise automatique** en cas de panne.\r\n>\r\n> De leur côté, des solutions commerciales comme **NGINX Plus** offrent également des fonctionnalités étendues, incluant une gestion fine des sessions, des métriques en temps réel, ou encore le support natif de protocoles spécifiques.\r\n\r\nEn résumé, le **load balancing** est un composant essentiel des architectures modernes, garantissant **performance, fiabilité et scalabilité**. Qu’il soit implémenté par des solutions logicielles comme **NGINX** ou **HAProxy**, ou par du matériel spécialisé, il joue un rôle de chef d’orchestre entre les utilisateurs et les serveurs. Le choix du bon algorithme ou de la bonne solution dépendra toujours du **contexte d’usage**, du **budget**, et des **contraintes techniques**.","src/data/blog/load-balancer.mdx","657aadf4de2c36ea","qu-est-ce-que-design-pattern",{"id":156,"data":158,"body":167,"filePath":168,"digest":169,"deferredRender":89},{"title":159,"contentType":77,"isDraft":78,"taxonomies":160,"thumbnail":163,"summary":164,"pubDate":165,"author":166},"Qu’est-ce qu’un design pattern ?",[161,162],{"id":17,"collection":9},{"id":57,"collection":9},"/design_pattern.png","Les design patterns sont des solutions éprouvées et validées par les pionniers de l’informatique. Ils ont été conçus pour structurer et organiser le code de manière claire, lisible et efficace.",["Date","2025-07-12T00:00:00.000Z"],{"id":64,"collection":62},"import BlockDesignPattern from \"../../components/blog/pattern/BlockDesignPattern.tsx\";\r\nimport DisplayList from \"../../components/blog/pattern/DisplayList.tsx\";\r\n\r\nLes **design patterns** sont des solutions éprouvées et validées par les\r\npionniers de l'informatique. Ils ont été conçus pour **structurer et organiser\r\nle code** de manière claire, lisible et efficace. L’idée est de proposer des\r\n**modèles génériques et réutilisables**, qui facilitent la **scalabilité** et\r\nla **maintenance** des systèmes logiciels, tout en réduisant les erreurs\r\ncourantes.\r\n\r\n## histoire\r\n\r\nLes **design patterns** sont nés de la volonté de produire du code\r\n**scalable** et maintenable, même si cela signifiait, au départ, **sacrifier\r\nun peu de confort de développement**. Lorsque les équipes techniques\r\nremarquent qu’une même structure ou solution est **répétée plusieurs fois dans\r\nle code** pour résoudre un même type de problème, elles finissent par lui\r\ndonner un **nom commun**. C’est ainsi qu’est née l’idée des **design\r\npatterns** : des solutions **nommées, génériques et réutilisables**,\r\ndocumentées pour faciliter la collaboration et la compréhension dans les\r\nprojets logiciels.\r\n\r\n\u003Cblockquote>\r\n  \u003Cem>\r\n    historiquement, Le concept de design pattern est inspiré du travail de\r\n    **Christopher Alexander** dans l’architecture, puis formalisé en\r\n    informatique par les **\"Gang of Four\"** en 1994. Ces patterns sont devenus\r\n    une base incontournable pour structurer le code, éviter les redondances, et\r\n    faciliter la maintenance des applications à grande échelle.\r\n  \u003C/em>\r\n\u003C/blockquote>\r\n\r\n## Pourquoi devrais je apprendre des modèles ?\r\n\r\nDans la majorité des cas, vous avez probablement déjà **utilisé des design\r\npatterns sans le savoir**. C’est exactement ce qui distingue souvent un\r\ndéveloppeur junior d’un développeur senior : la capacité à reconnaître,\r\ncomprendre et appliquer consciemment ces structures. Par exemple, le **code\r\nsource des Framework** (comme React, Laravel, AdonisJS, etc.) en est truffé.\r\n\r\nUne fois maîtrisés, les **design patterns** deviennent une véritable **boîte à\r\noutils** pour résoudre efficacement des problèmes courants en développement.\r\nIls vous aideront à **penser autrement**, avec plus de structure et de recul.\r\nAvec le temps, cette manière de réfléchir deviendra presque naturelle.\r\n\r\n## Classifications et Rôle:\r\n\r\nCette liste n’est pas exhaustive et ne couvre pas l’ensemble des design\r\npatterns que vous pourriez rencontrer. Nous allons nous concentrer sur un type\r\nparticulier de modèles : les **modèles architecturaux**.\r\n\r\nCes modèles peuvent être implémentés dans **n’importe quel langage de\r\nprogrammation** et sont appelés **modèles universels** ou **patterns de haut\r\nniveau**. Ils s’opposent aux **idiomes**, qui sont des solutions plus\r\nspécifiques, propres à un langage particulier et souvent de bas niveau.\r\n\r\n\u003Cblockquote>\r\n- Les **patterns architecturaux** définissent la structure globale d’une\r\n  application ou d’un système (ex. MVC, Client-Serveur, Microservices).\r\n\r\n- Les **patterns de conception** (design patterns au sens plus classique) concernent\r\n  l’organisation du code à l’intérieur des composants (ex. Singleton, Factory,\r\n  Observer).\r\n\r\n- Les **idiomes** sont des constructions propres à un langage, qui\r\n  exploitent ses spécificités syntaxiques et sémantiques.\r\n\r\n \u003C/blockquote>\r\n\r\n\u003CDisplayList client:only />\r\n\r\n---\r\n\r\n## Cas concret : un store Zustand refactoré selon les Design Patterns\r\n\r\nPour illustrer concrètement l’intérêt des design patterns, prenons un\r\n**exemple réel** rencontré lors de la construction de mon application.\r\nJ’utilise Zustand comme **solution de gestion d’état** globale. C’est un outil\r\nsimple, mais très puissant, qui permet de construire un store à la volée.\r\n\r\nCependant, en avançant dans le développement, le store devient **complexe,\r\ndifficile à tester ou à maintenir**, et certaines logiques sont dupliquées.\r\nC’est exactement le genre de situation où **les design patterns prennent tout\r\nleur sens**.\r\n\r\nDans cette section, nous allons **refactorer ce store pas à pas**, en\r\nappliquant différents design patterns. Chaque refactor sera **associé à un\r\npattern précis** : Singleton, Factory, Observer, etc. Cela permettra de\r\ncomprendre **à la fois la théorie** derrière chaque modèle **et son\r\napplication pratique** dans un contexte moderne (React + Zustand).\r\n\r\n## Le code de base\r\n\r\nAvant de commencer le refactor, voici **le store Zustand tel qu’il existait\r\ninitialement** dans mon projet. Il est fonctionnel, et implémentait\r\ncertainspattern partiellement\r\n\r\n- Factory Method (partiellement)\r\n- Strategy\r\n\r\n```ts\r\nimport { UnAuthenticatedError } from \"@helpers/website\";\r\n\r\nimport {\r\n  createContext,\r\n  useContext,\r\n  useMemo,\r\n  type PropsWithChildren,\r\n} from \"react\";\r\n\r\nimport { create, useStore as useZustandStore } from \"zustand\";\r\n\r\nimport { combine, persist } from \"zustand/middleware\";\r\n\r\nimport type { Account } from \"./hooks/useAuth.ts\";\r\n\r\nimport type {\r\n  AccessLevels,\r\n  Courses,\r\n  Difficulties,\r\n  Statuses,\r\n} from \"@api/website/types\";\r\n\r\nexport type ResourceMap = {\r\n  accessLevel: AccessLevels;\r\n\r\n  difficulties: Difficulties;\r\n\r\n  statuses: Statuses;\r\n};\r\n\r\ntype State = {\r\n  account: undefined | null | Record\u003Cstring, any>;\r\n\r\n  organization: Record\u003Cstring, any>;\r\n\r\n  accesslevels: AccessLevels[];\r\n\r\n  difficulties: Difficulties[];\r\n\r\n  statuses: Statuses[];\r\n\r\n  courses: Courses[];\r\n};\r\n\r\nfunction getStateKey\u003CT extends keyof ResourceMap>(\r\n  type: T\r\n): keyof Omit\u003CState, \"account\" | \"organization\" | \"courses\"> {\r\n  switch (type) {\r\n    case \"accessLevel\":\r\n      return \"accesslevels\";\r\n\r\n    case \"difficulties\":\r\n      return \"difficulties\";\r\n\r\n    case \"statuses\":\r\n      return \"statuses\";\r\n\r\n    default:\r\n      throw new Error(\"Courses resource type \" + type);\r\n  }\r\n}\r\n\r\nconst createStore = () =>\r\n  create(\r\n    persist(\r\n      combine(\r\n        {\r\n          account: undefined as undefined | null | Account,\r\n\r\n          organization: {},\r\n\r\n          courses: [],\r\n\r\n          accesslevels: [],\r\n\r\n          difficulties: [],\r\n\r\n          statuses: [],\r\n        } as State,\r\n\r\n        (set) => ({\r\n          setResources: function \u003CT extends keyof ResourceMap>(\r\n            type: T,\r\n\r\n            data: ResourceMap[T][]\r\n          ) {\r\n            const key = getStateKey(type);\r\n\r\n            return set({ [key]: data });\r\n          },\r\n\r\n          addResource: function \u003CT extends keyof ResourceMap>(\r\n            type: T,\r\n\r\n            newData: ResourceMap[T]\r\n          ) {\r\n            const key = getStateKey(type);\r\n\r\n            return set((state) => ({\r\n              [key]: [...state[key], newData],\r\n            }));\r\n          },\r\n\r\n          updateResource: function \u003CT extends keyof ResourceMap>(\r\n            type: T,\r\n\r\n            newData: ResourceMap[T]\r\n          ) {\r\n            const key = getStateKey(type);\r\n\r\n            return set((state) => ({\r\n              [key]: state[key].map((item) =>\r\n                item.id === newData.id ? { ...item, ...newData } : item\r\n              ),\r\n            }));\r\n          },\r\n\r\n          deleteResource: function \u003CT extends keyof ResourceMap>(\r\n            type: T,\r\n\r\n            id: number\r\n          ) {\r\n            const key = getStateKey(type);\r\n\r\n            return set((state) => ({\r\n              [key]: state[key].filter((item) => item.id !== id),\r\n            }));\r\n          },\r\n\r\n          setCourses: (courses: Courses[]) => {\r\n            set({ courses });\r\n          },\r\n\r\n          addCourse: (course: Courses) => {\r\n            set((state) => ({\r\n              courses: [...state.courses, course],\r\n            }));\r\n          },\r\n\r\n          updateOrganization: (newDate: Record\u003Cstring, any>) =>\r\n            set({ organization: newDate }),\r\n\r\n          updateAccount: (account: Account | null) => set({ account }),\r\n        })\r\n      ),\r\n\r\n      {\r\n        name: \"account\",\r\n      }\r\n    )\r\n  );\r\n\r\ntype Store = ReturnType\u003Ctypeof createStore>;\r\n\r\ntype StoreState = Store extends {\r\n  getState: () => infer T;\r\n}\r\n  ? T\r\n  : never;\r\n\r\nconst StoreContext = createContext\u003C{ store?: Store }>({});\r\n\r\nexport function StoreProvider({ children }: PropsWithChildren) {\r\n  const store = useMemo(() => createStore(), []);\r\n\r\n  return (\r\n    \u003CStoreContext.Provider value={{ store: store }}>\r\n            {children}   {\" \"}\r\n    \u003C/StoreContext.Provider>\r\n  );\r\n}\r\n\r\nexport function useStore\u003CT>(selector: (state: StoreState) => T) {\r\n  const store = useContext(StoreContext).store;\r\n\r\n  if (!store) {\r\n    throw new Error(\"A context need to be provider to use the store\");\r\n  }\r\n\r\n  return useZustandStore(store, selector);\r\n}\r\n\r\nexport type InferResourceType\u003CT> = T extends keyof ResourceMap\r\n  ? ResourceMap[T]\r\n  : never;\r\n\r\nexport function useResource\u003CT extends keyof ResourceMap>(type: T) {\r\n  const key = getStateKey(type);\r\n\r\n  const list = useStore((state) => state[key]) as InferResourceType\u003CT>[];\r\n\r\n  const setResources = useStore((state) => state.setResources);\r\n\r\n  const addResource = useStore((state) => state.addResource);\r\n\r\n  const updateResource = useStore((state) => state.updateResource);\r\n\r\n  const deleteResource = useStore((state) => state.deleteResource);\r\n\r\n  return {\r\n    list,\r\n\r\n    set: (data: InferResourceType\u003CT>[]) => setResources(type, data),\r\n\r\n    add: (data: InferResourceType\u003CT>) => addResource(type, data),\r\n\r\n    update: (data: InferResourceType\u003CT>) => updateResource(type, data),\r\n\r\n    delete: (id: number) => deleteResource(type, id),\r\n  };\r\n}\r\n\r\n// ACCESS_LEVELS\r\n\r\nexport function useAccessLevels() {\r\n  return useResource(\"accessLevel\");\r\n}\r\n\r\n// DIFFICULTIES\r\n\r\nexport function useDifficulties() {\r\n  return useResource(\"difficulties\");\r\n}\r\n\r\n// STATUSES\r\n\r\nexport function useStatuses() {\r\n  return useResource(\"statuses\");\r\n}\r\n\r\n// COURSES\r\n\r\nexport function useCourses() {\r\n  const list = useStore((state) => state.courses);\r\n\r\n  const setCourses = useStore((state) => state.setCourses);\r\n\r\n  const addCourses = useStore((state) => state.addCourse);\r\n\r\n  return {\r\n    list,\r\n\r\n    set: (data: Courses[]) => setCourses(data),\r\n\r\n    add: (data: Courses) => addCourses(data),\r\n  };\r\n}\r\n\r\n// ORGANISATION\r\n\r\nexport function useOrganization() {\r\n  return useStore((state) => state.organization);\r\n}\r\n\r\nexport function useUpdateOrganization() {\r\n  return useStore((state) => state.updateOrganization);\r\n}\r\n\r\nexport function useUpdateAccount() {\r\n  return useStore((state) => state.updateAccount);\r\n}\r\n\r\nexport function useIsAuth() {\r\n  const account = useStore((state) => state.account);\r\n\r\n  if (!account) {\r\n    throw new UnAuthenticatedError();\r\n  }\r\n\r\n  return {\r\n    ...account,\r\n  };\r\n}\r\n\r\nexport function useAccount() {\r\n  const account = useStore((state) => state.account);\r\n\r\n  return {\r\n    ...account,\r\n  };\r\n}\r\n```\r\n\r\n### Objectifs de la version refactorée\r\n\r\n- Séparer les responsabilités.\r\n- Appliquer des patterns classiques.\r\n- Garder une API propre et extensible.\r\n\r\n## Singleton:\r\n\r\nUn singleton s'assure d'avoir qu'une seul instance d'un object (de préférence\r\nune class) ne soit initialisé, offrant ainsi un seul point global\r\nd'initialisation. Dans notre situation nous somme en javascript ou chaque\r\nobject et module est unique dans son contexte d'exécution.\r\n\r\n— **Singleton Pattern**\r\n\r\n```ts\r\nimport { create } from \"zustand\";\r\nimport { combine, persist } from \"zustand/middleware\";\r\nimport type { State, Store, ResourceKey, InferResourceType } from \"./types\";\r\nimport { getStateKey } from \"./factory\";\r\n\r\nlet storeInstance: Store | undefined;\r\n\r\nexport const createStore = (): Store => {\r\n  if (storeInstance) return storeInstance;\r\n\r\n  storeInstance = create(\r\n    persist(\r\n      combine(\r\n        {\r\n          account: undefined,\r\n          organization: {},\r\n          courses: [],\r\n          accesslevels: [],\r\n          difficulties: [],\r\n          statuses: [],\r\n        } as State,\r\n        (set) => ({\r\n          updateAccount: (account) => set({ account }),\r\n          updateOrganization: (org) => set({ organization: org }),\r\n\r\n          setResources: \u003CT extends ResourceKey>(\r\n            type: T,\r\n            data: InferResourceType\u003CT>[]\r\n          ) => set({ [getStateKey(type)]: data }),\r\n\r\n          addResource: \u003CT extends ResourceKey>(\r\n            type: T,\r\n            item: InferResourceType\u003CT>\r\n          ) =>\r\n            set((state) => ({\r\n              [getStateKey(type)]: [...state[getStateKey(type)], item],\r\n            })),\r\n\r\n          updateResource: \u003CT extends ResourceKey>(\r\n            type: T,\r\n            item: InferResourceType\u003CT>\r\n          ) =>\r\n            set((state) => ({\r\n              [getStateKey(type)]: state[getStateKey(type)].map((i) =>\r\n                i.id === item.id ? { ...i, ...item } : i\r\n              ),\r\n            })),\r\n\r\n          deleteResource: \u003CT extends ResourceKey>(type: T, id: number) =>\r\n            set((state) => ({\r\n              [getStateKey(type)]: state[getStateKey(type)].filter(\r\n                (i) => i.id !== id\r\n              ),\r\n            })),\r\n        })\r\n      ),\r\n      { name: \"account\" }\r\n    )\r\n  );\r\n\r\n  return storeInstance;\r\n};\r\n```\r\n\r\nIci on assure qu’un seul store Zustand existe dans l’app, ce qui est important\r\npour éviter les incohérences ou re-rendu inutile dans React `createStore()`\r\ndans le contexte React.\r\n\r\nje ne reviendrais pas sur l'utilisation de Zustand, dans un prochain article.\r\nEn bref :\r\n\r\n- **Combine** : est un middleware qui permet de séparer le state et les actions.\r\n- **Persist** : est un middleware qui permet de faire de la persistance avec le local Storage.\r\n\r\n## factory\r\n\r\n— **Factory Pattern pour les clés**\r\n\r\n```ts\r\nexport const getStateKey = \u003CT extends ResourceKey>(type: T): keyof State => {\r\n  const map: Record\u003CResourceKey, keyof State> = {\r\n    accessLevel: \"accesslevels\",\r\n    difficulties: \"difficulties\",\r\n    statuses: \"statuses\",\r\n  };\r\n  const key = map[type];\r\n  if (!key) throw new Error(`Unknown resource type: ${type}`);\r\n  return key;\r\n};\r\n```\r\n\r\nOn **abstrait la logique** de mappage `\"accessLevel\"` → `\"accesslevels\"` dans\r\nun objet **déclaratif**, au lieu d’un `switch`.\r\n\r\n## Facade\r\n\r\n— **Facade Pattern**\r\n\r\n```ts\r\nexport const useAccount = () => {\r\n  const account = useStore((s) => s.account);\r\n  return { ...account };\r\n};\r\n```\r\n\r\nOn caches la complexité du store et exposes une API simple.\r\n\r\n## Illustrations en pseudo-code\r\n\r\n\u003Cblockquote>\r\n  Bien que cet article ait pour objectif de fournir une **implémentation\r\n  concrète** des design patterns dans un contexte réel (React + Zustand),\r\n  certains modèles comme **Singleton** ou **Factory Method** s’intègrent\r\n  naturellement dans l’architecture de mon store. En revanche, d’autres modèles\r\n  comme **Builder**, **Strategy** ou **Decorator** sont plus **conceptuels**\r\n  dans ce contexte. Ils seront donc illustrés de manière plus **générique en\r\n  pseudo-code** pour faciliter leur compréhension. Ces exemples ne sont **pas\r\n  destinés à être copiés tels quels** dans un projet Zustand ou React, mais\r\n  plutôt à vous aider à **saisir l’idée générale** derrière chaque pattern. Vous\r\n  verrez ensuite comment **adapter ces concepts** dans un projet réel si\r\n  nécessaire.\r\n\u003C/blockquote>\r\n\r\n### Builder (construire un objet étape par étape)\r\n\r\n```ts\r\nclass CourseBuilder {\r\n  name = \"\";\r\n  color = \"\";\r\n\r\n  setName(name: string) {\r\n    this.name = name;\r\n    return this;\r\n  }\r\n\r\n  setColor(color: string) {\r\n    this.color = color;\r\n    return this;\r\n  }\r\n\r\n  build() {\r\n    return { name: this.name, color: this.color };\r\n  }\r\n}\r\n\r\nconst course = new CourseBuilder().setName(\"React\").setColor(\"blue\").build();\r\n```\r\n\r\n### Strategy (changer de comportement dynamiquement)\r\n\r\n```ts\r\nclass ExportStrategy {\r\n  execute(data) {\r\n    throw \"Not implemented\";\r\n  }\r\n}\r\n\r\nclass JsonExport extends ExportStrategy {\r\n  execute(data) {\r\n    return JSON.stringify(data);\r\n  }\r\n}\r\n\r\nclass CsvExport extends ExportStrategy {\r\n  execute(data) {\r\n    return data.map((row) => row.join(\",\")).join(\"\\n\");\r\n  }\r\n}\r\n\r\nfunction exportData(data, strategy: ExportStrategy) {\r\n  return strategy.execute(data);\r\n}\r\n```\r\n\r\n### Decorator (enrichir un comportement sans toucher au code source)\r\n\r\n```ts\r\nfunction withLogger(fn) {\r\n  return function (...args) {\r\n    return fn(...args);\r\n  };\r\n}\r\n\r\nfunction saveCourse(course) {}\r\n\r\nconst loggedSaveCourse = withLogger(saveCourse);\r\n\r\nloggedSaveCourse({ name: \"JS\", color: \"yellow\" });\r\n```\r\n\r\n\u003Chr />\r\n\r\n## Conclusion\r\n\r\nLes **design patterns** sont des outils puissants, à condition d’être utilisés\r\ndans le **bon contexte** et de manière réfléchie. On peut y penser **en\r\namont**, lors de la conception, si l’on est à l’aise, ou bien **les introduire\r\nprogressivement** en refactorant le projet au fil du temps.\r\n\r\nIls permettent d’**éviter la répétition**, de **faire évoluer** le code plus\r\nfacilement, de **l’améliorer** et surtout de **mieux le tester**.\r\n\r\nDans cet article, nous avons vu comment **certains modèles** comme le\r\n**Singleton**, la **Factory Method**, ou la **Facade** peuvent s’appliquer\r\n**directement** dans une architecture moderne comme React + Zustand. D’autres\r\npatterns plus **conceptuels** (Builder, Strategy, Decorator) ont été illustrés\r\nsous forme de **pseudo-code** afin de mieux saisir leur intention.\r\n\r\n### En bref :\r\n\r\n- Les patterns ne sont pas une contrainte, mais une **liberté maîtrisée**.\r\n- Ils vous permettent d’**éviter les pièges classiques** du développement à mesure que vos projets prennent de l’ampleur.\r\n- **Apprendre à reconnaître** et à utiliser ces modèles, c’est aussi progresser en **maturité logicielle**.","src/data/blog/qu-est-ce-que-design-pattern.mdx","7b26ef0a1e09f671","en/global-id",{"id":170,"data":172,"body":178,"filePath":179,"digest":180,"deferredRender":89},{"title":93,"contentType":77,"isDraft":78,"taxonomies":173,"thumbnail":96,"summary":175,"pubDate":176,"author":177},[174],{"id":11,"collection":9},"Identifier generation in applications represents a fundamental business requirement. Whether creating purchase orders, users, or any other business entity, each object requires a unique and traceable identifier.",["Date","2025-09-04T00:00:00.000Z"],{"id":64,"collection":62},"Identifier generation in applications represents a fundamental business requirement. Whether creating purchase orders, users, or any other business entity, each object requires a **unique and traceable** identifier. This requirement becomes critical when the application begins to **scale horizontally** (instance multiplication, microservices, etc.).\r\n\r\nIn conventional applications, auto-incremented identifiers or UUIDs are commonly employed. However, these methods present limitations:\r\n\r\n- Incremental identifiers are **highly predictable** (which may enable, for example, estimation of total order volumes or access to unauthorized resources).\r\n- UUIDs, while marginally more secure, may prove excessively long, insufficiently optimized for certain databases, or lacking **business structure**.\r\n\r\nA more structured and secure approach thus becomes necessary to generate **robust, non-predictable identifiers compatible with distributed architecture**.\r\n\r\n## Limitations of Conventional Approaches\r\n\r\n### Auto-Incrementation\r\n\r\n- Highly predictable: these identifiers follow strict sequential ordering, rendering them easily guessable. An attacker may thus estimate data volumes (order quantities, user counts, etc.), or attempt unauthorized access through simple incrementation.\r\n\r\n### UUID (notably UUIDv4)\r\n\r\n- **Elevated storage requirements**: a UUIDv4 occupies 16 bytes, compared to 4 bytes for an auto-incremented integer (INT). This overhead impacts:\r\n  - database performance (indexing, sorting, searching)\r\n  - application-side memory utilization, particularly at scale\r\n- **Poor readability and manipulation difficulties**:\r\n  - UUIDs are lengthy, lacking readable structure and difficult for human interpretation.\r\n  - they complicate debugging, interface display, or clean URL construction.\r\n  - they convey no business information (entity type, creation date, tenant, etc.).\r\n- **Theoretical collision risk**:\r\n  - while rare, collisions remain possible with deficient randomness sources.\r\n  - despite vast identifier space (128 bits → 3.4 × 10³⁸ possibilities), very high-volume applications may reach thresholds where these risks become tangible.\r\n\r\n## GID Presentation\r\n\r\n### Definition\r\n\r\nA **Global ID (GID)** constitutes a unique representation of an entity within an application. It enables reliable and traceable identification of any business object (user, order, document, etc.), while accounting for **tenant context** (for example, an organization or client in a multi-tenant application).\r\n\r\n**Note:** a _tenant_ represents an organization or client in a _multi-tenant_ application (i.e., an application utilized by multiple organizations).\r\n\r\n### Objective and Justification\r\n\r\nIn this article, we will work with a **24-byte** GID. This format is non-standard: most technologies utilize **16 bytes**, as defined by standards **ISO/IEC 11578:1996**, **ITU-T Rec. X.667**, and **RFC 4122** (UUID specification).\r\n\r\nWe choose a **24-byte (192-bit)** GID to address specific requirements:\r\n\r\n- guaranteeing **reinforced global uniqueness**,\r\n- directly incorporating **business information** (tenant, entity type, timestamp, etc.) within the identifier structure.\r\n- Optimizing for multi-tenancy: avoiding costly joins to identify an entity's tenant\r\n\r\n**Regarding performance impact:** while 24 bytes exceed standard UUID size (16 bytes), the integrated business information enables avoiding numerous SQL joins, substantially offsetting this overhead. Additionally, indexing remains efficient thanks to the predictable structure.\r\n\r\n## GID Anatomy (Binary Explanation)\r\n\r\n### Fundamental Concepts\r\n\r\n- **Bit**: the smallest data unit, representing a binary value (0 or 1).\r\n- **Byte**: a grouping of 8 bits, sufficient for representing a character or complete data element.\r\n\r\nBy analogy:\r\n\r\n- a **bit** resembles an elementary letter\r\n- a **byte** represents the smallest \"word\" the computer can directly manipulate.\r\n\r\nTherefore:\r\n\r\n- 2 bytes = 16 bits (2 × 8)\r\n- 8 bytes = 64 bits (8 × 8)\r\n- 24 bytes = 192 bits (24 × 8)\r\n\r\n**General formula:**\r\n\r\n\u003Cimg src=\"/global_id/formule.png\" alt=\"General formula\" />\r\n\r\n## Practical Implementation\r\n\r\nWe will implement a **24-byte GID** with Go, in a **multi-tenant system**. Each identifier block receives precise allocation to constitute the 24 bytes\r\n\r\n| Bytes | Content     | Size              |\r\n| ----- | ----------- | ----------------- |\r\n| 0-7   | Tenant ID   | 8 bytes (64 bits) |\r\n| 8-9   | Entity Type | 2 bytes (16 bits) |\r\n| 10-17 | Timestamp   | 8 bytes (64 bits) |\r\n| 18-23 | Random      | 6 bytes (48 bits) |\r\n\r\nThus, the GID contains both business information (tenant, type), a timestamp for traceability, and random data guaranteeing global uniqueness.\r\n\r\n### 1. Variable Declaration\r\n\r\n```go\r\npackage gid\r\n\r\nimport (\r\n    \"crypto/rand\"\r\n    \"encoding/binary\"\r\n    \"encoding/hex\"\r\n    \"fmt\"\r\n    \"strings\"\r\n    \"time\"\r\n)\r\n\r\nconst (\r\n    GIDSize = 24 // 192 bits total\r\n)\r\n\r\ntype (\r\n    GID      [GIDSize]byte // GID = 24-byte array\r\n    TenantID [8]byte       // Tenant on 8 bytes\r\n)\r\n```\r\n\r\n### 2. GID Initialization\r\n\r\n```go\r\npackage gid\r\n\r\nfunc NewGID(tenantID TenantID, entityType uint16) {\r\n    var id GID // initialize a 24-byte array for our GID\r\n}\r\n```\r\n\r\n### 3. Block Separation and Construction\r\n\r\n```go\r\npackage gid\r\n\r\nfunc NewGID(tenantID TenantID, entityType uint16) (GID, error) {\r\n    var id GID\r\n\r\n    // 1. Tenant ID (bytes 0 to 7)\r\n    // Copy the 8 bytes from tenantID into the first 8 bytes of id\r\n    copy(id[0:8], tenantID[:])\r\n\r\n    // 2. Entity Type (bytes 8 to 9)\r\n    // - entityType is a uint16 (16 bits = 2 bytes)\r\n    // - Encode it as Big Endian bytes and write it to id[8] and id[9]\r\n    binary.BigEndian.PutUint16(id[8:10], entityType)\r\n\r\n    // 3. Timestamp (bytes 10 to 17)\r\n    // - Retrieve the current timestamp in milliseconds\r\n    // - Convert it to uint64 (64 bits = 8 bytes)\r\n    // - Write these 8 bytes to id[10] through id[17]\r\n    now := time.Now().UnixMilli()\r\n    binary.BigEndian.PutUint64(id[10:18], uint64(now))\r\n\r\n    // 4. Random Data (bytes 18 to 23)\r\n    // - These 6 bytes serve to guarantee global GID uniqueness\r\n    // - Fill them with random bytes\r\n    _, err := rand.Read(id[18:24])\r\n    if err != nil {\r\n        return Nil, fmt.Errorf(\"failed to generate random bytes: %v\", err)\r\n    }\r\n\r\n    return id, nil\r\n}\r\n```\r\n\r\n## GID Advantages\r\n\r\n1. **Native traceability**: integrated timestamp for auditing and debugging\r\n2. **Context-aware**: tenant and entity type directly accessible\r\n3. **Performance**: avoids joins to identify context\r\n4. **Security**: non-predictable thanks to random component\r\n5. **Flexibility**: structured yet extensible format\r\n\r\n## Conclusion\r\n\r\nThe code presented here is Go-specific, but the **GID concept** can be transposed to any programming language.\r\n\r\nThe primary objective is to **visualize how to represent a Global Identifier (GID) informatively**, by grouping **concrete business data** (tenant, entity type, timestamp) while guaranteeing:\r\n\r\n- **global uniqueness**,\r\n- and **predictability resistance**.\r\n\r\nThus, the GID enables creating identifiers that are simultaneously **robust, traceable, and business-useful**, while remaining **optimized for distributed or multi-tenant architecture**.","src/data/blog/en/global-id.mdx","d00ea020055186ba","en/convolutional-neural-networks",{"id":181,"data":183,"body":191,"filePath":192,"digest":193,"deferredRender":89},{"title":184,"contentType":77,"isDraft":78,"taxonomies":185,"thumbnail":82,"summary":188,"pubDate":189,"author":190},"Convolutional Neural Networks",[186,187],{"id":52,"collection":9},{"id":23,"collection":9},"A Convolutional Neural Network (CNN) is a type of supervised deep learning algorithm that confers upon computers a form of 'vision': it can analyze and recognize objects in the visual world.",["Date","2025-08-12T00:00:00.000Z"],{"id":64,"collection":62},"**From sports to medicine, transportation to security...**  \r\nHave you ever wondered how a surveillance camera manages to recognize a face in a crowd, how a medical scanner detects a concealed tumor, or how a system tracks a train's trajectory with millimeter precision?\r\n\r\n## Introduction\r\n\r\nA **Convolutional Neural Network** (_Convolutional Neural Network_, or CNN) is a type of supervised deep learning algorithm that confers upon computers a form of \"vision\": it can analyze and recognize objects in the visual world.\r\n\r\nThe **Convolutional Neural Network** (_Convolutional Neural Network_, CNN), by analogy, refers to the functioning of biological neural networks and consequently constitutes an extension of the Artificial Neural Network (_Artificial Neural Network_, ANN). Its applications are primarily found in image recognition and analysis.\r\n\r\nBuilding upon the principles of linear algebra, and more particularly on matrix manipulation, convolutional neural networks apply convolution and transformation operations to detect and extract pertinent patterns within an image.\r\n\r\n## How CNNs Operate\r\n\r\nThe functioning of a **Convolutional Neural Network** (CNN) relies on an architecture composed of three principal layer types. These layers process spatially structured input data, such as images, audio spectrograms, or video sequences:\r\n\r\n- **Convolutional layer** (_Convolutional layer_)\r\n- **Pooling or subsampling layer** (_Pooling layer_)\r\n- **Fully connected layer** (_Fully connected layer_)\r\n\r\n## **Convolutional Layer**\r\n\r\nThe convolutional layer constitutes the network's core, where the majority of computations are performed. Its operation requires components such as:\r\n\r\n- Input data\r\n- Filter (kernel)\r\n- Feature map\r\n\r\n### Operational Mechanism:\r\n\r\n- **Convolution** (dot product)\r\n- **ReLU** (eliminates negative values)\r\n- **Pooling** (dimensionality reduction)\r\n- Repeat these steps\r\n- **Final classification**\r\n\r\n**Input Data**: Consider an input image of size 24×24 pixels, which will be transformed into a 3D pixel matrix corresponding to the respective number of pixels in the image. This means that the input is represented in height, width, and depth corresponding to the RGB characteristics of the image.\r\n\r\n**Filter (kernel)**: The filter is a weight matrix that traverses the image to detect shapes and characteristics. This process is known as the convolution operation.\r\n\r\n- Contours\r\n- Lines\r\n- Textures\r\n- Geometric shapes\r\n- More complex characteristics in deeper layers\r\n\r\nIts size may vary, although by default it is 3×3, there exist 5×5, or 7×7 pixel filters. The **receptive field** corresponds to the input image area that influences an output neuron.\r\n\r\nThe filter is positioned on an image region and the dot product is calculated based on the input pixels and the filter. This value is then saved in a feature map. Subsequently, the filter is shifted by one stride, repeating the process until the entire pixel surface of the image has been processed.\r\n\r\nCertain filter hyperparameters remain fixed during the process, however the **weights** are adjusted to maximize performance via backpropagation and gradient descent operations. Three critical hyperparameters exist that are paramount in obtaining favorable results and that influence the output volume; they must be defined well before the process.\r\n\r\n1. **Number of filters**: Affects the depth of the output. For example, three distinct filters would produce three different feature maps, thus creating a depth of three. _Each filter acts as a specialized detector (contours, textures, shapes) and produces its own feature map; these maps stack to form the output depth._\r\n\r\n2. **Stride**: Is the distance, or number of pixels, over which the kernel moves across the input matrix. While stride values of two or more are rare, a larger stride produces a smaller output. _A stride of 1 means the filter moves pixel by pixel (detailed examination), while a stride of 2 skips one pixel with each movement (faster examination, smaller image)._\r\n\r\n3. **Zero-padding**: Is generally used when filters do not match the input image. This zeros all elements located outside the input matrix, producing a larger or equal-sized output. Three padding types exist:\r\n\r\n   - **Valid padding:** This is also called the absence of padding. In this case, the last convolution is abandoned if dimensions do not align. _(If the puzzle pieces don't fit at the edge, they are abandoned)_\r\n   - **Same padding:** This padding guarantees that the output layer has the same size as the input layer. _(Empty pieces are added around so everything fits perfectly)_\r\n   - **Full padding:** This padding type increases the output size by adding zeros to the input border. _(Even more empty pieces are added to obtain a larger image)_\r\n\r\n**Following the convolution operation, the CNN immediately applies the ReLU function** (**Re**ctified **L**inear **U**nit). This activation function acts as a selective filter that eliminates weak signals (negative values) to retain only significant activations (positive values).\r\n\r\n\u003Cimg src=\"/Relu-activation-function.webp\" alt=\"ReLU activation function\" />\r\n\r\n**Operational Principle:**\r\n\r\n- Positive value → retained as is\r\n- Negative value → transformed to zero\r\n\r\n**Formula:** ReLU(x) = max(0, x)\r\n\r\n**Practical Example:**\r\nFeature map before ReLU: `[-2, 5, -1, 8, -3, 4]`\r\nFeature map after ReLU: `[0, 5, 0, 8, 0, 4]`\r\n\r\n**Impact on Learning:** The introduction of this non-linearity is crucial as it enables the network to model complex relationships between characteristics. Without ReLU, the CNN would be limited to linear transformations and could not detect sophisticated patterns such as spatial relationships between facial elements (eye-nose distance, mouth-cheek configuration) or other complex visual subtleties.\r\n\r\nThis step thus transforms a simple mathematical calculation into a genuine intelligent recognition process.\r\n\r\n\u003Cimg\r\n  src=\"/iclh-diagram-convolutional-neural-networks.png\"\r\n  alt=\"Convolutional neural networks diagram\"\r\n/>\r\n\r\n### Additional Convolutional Layer\r\n\r\nIt is frequent that layer hierarchization is applied to progressively analyze the increasing complexity of an image. This cascade architecture enables the CNN structure to adapt intelligently: subsequent layers can exploit information from the receptive fields of preceding layers, thus creating a genuine detection hierarchy.\r\n\r\n**Characteristic Hierarchy Principle:**\r\n\r\n- **Initial layers**: Detect low-level characteristics (contours, lines, simple textures)\r\n- **Intermediate layers**: Combine these elements to identify more complex shapes (angles, curves, geometric patterns)\r\n- **Deep layers**: Recognize specific object parts (wheels, handlebars, frames)\r\n- **Final layers**: Assemble these parts to identify the complete object (bicycle, car, face)\r\n\r\n**Concrete Example:** Consider bicycle recognition. The CNN proceeds in stages:\r\n\r\n1. Detection of circles and straight lines\r\n2. Identification of circular shapes (potential wheels)\r\n3. Recognition of handlebars and frame\r\n4. Final association: \"wheels + handlebars + frame = bicycle\"\r\n\r\nThis hierarchical approach enables the network to construct a progressive understanding of the image, where each layer refines and enriches the analysis of the previous one. Ultimately, the convolutional layer converts the image into structured numerical values, enabling the neural network to interpret and extract pertinent patterns for final classification.\r\n\r\n\u003Cimg src=\"/hierarchy.png\" alt=\"CNN characteristic hierarchy\" />\r\n\r\n## **Pooling or Subsampling Layer** (_Pooling layer_)\r\n\r\nIn this stage of the CNN process, the collected information is reduced and grouped to decrease data dimensionality. Following the same process as the convolutional layer, it applies a filter that, unlike the convolutional layer, has no weights. An aggregation function is applied to the receptive field values to populate the output array. **This aggregation function constitutes the primary configurable parameter that determines the pooling type utilized.**\r\n\r\n**Primary Objectives:**\r\n\r\n- Reduce data size\r\n- Decrease the number of parameters\r\n- Preserve important characteristics\r\n\r\n**The two principal pooling types (aggregation functions):**\r\n\r\n**1. Max Pooling:**\r\n\r\n- Aggregation function: `f(region) = max(values)`\r\n- Selects the **maximum value** in each receptive field region\r\n- Preserves the most salient characteristics\r\n- More commonly used as it preserves important contours and details\r\n\r\n**2. Average Pooling:**\r\n\r\n- Aggregation function: `f(region) = average(values)`\r\n- Calculates the **arithmetic mean** of all values in the receptive field\r\n- Smooths data by reducing noise\r\n- Less utilized but useful for certain specific applications\r\n\r\n## Visualization\r\n\r\n#### 2×2 region organized as a matrix:\r\n\r\n\u003Cimg\r\n  src=\"/Sans-titre-2025-06-23-2323 (2).png\"\r\n  alt=\"2x2 region organized as matrix\"\r\n/>\r\n\r\n## Calculations according to pooling type:\r\n\r\n**Max Pooling:**\r\n\r\n- We examine all values: `1, 3, 2, 4`\r\n- We take the **largest**: `max(1, 3, 2, 4) = 4`\r\n- **Result: 4**\r\n\r\n**Average Pooling:**\r\n\r\n- We sum all values: `1 + 3 + 2 + 4 = 10`\r\n- We divide by the number of values: `10 ÷ 4 = 2.5`\r\n- **Result: 2.5**\r\n\r\n## Process Visualization:\r\n\r\n\u003Cimg\r\n  src=\"/Sans-titre-2025-06-23-2323 (3).png\"\r\n  alt=\"Pooling process visualization\"\r\n/>\r\n\r\nThe pooling filter \"examines\" this 2×2 region and **summarizes it into a single value** according to the chosen aggregation function.\r\n\r\nThese parameters (aggregation function type, filter size, stride) are configurable before training according to the model's specific needs, enabling adaptation of pooling behavior to the targeted classification task.\r\n\r\n## **Fully Connected Layer** (_Fully connected layer_)\r\n\r\nThis layer performs classification based on characteristics extracted from previous layers and their filters. It applies an activation function (generally **softmax**) that enables data conversion to **probabilities**: each possible class receives a score between 0 and 1, and the **sum of all probabilities equals 1**.\r\n\r\n### Example:\r\n\r\nFor animal recognition:\r\n\r\n- Cat: 0.7 (70% probability)\r\n- Dog: 0.2 (20% probability)\r\n- Bird: 0.1 (10% probability)\r\n- **Total: 0.7 + 0.2 + 0.1 = 1.0**\r\n\r\nThe class with the **highest probability** (here \"Cat\" with 0.7) is the final prediction.\r\n\r\n## Conclusion\r\n\r\n**Convolutional Neural Networks** (CNNs) constitute a particularly effective deep learning architecture for image processing and analysis. Their operation relies on three principal components: convolutional layers that extract characteristics, pooling layers that reduce dimensionality, and fully connected layers that perform final classification.\r\n\r\n### Architecture and Performance\r\n\r\nThis hierarchical structure enables CNNs to progressively detect increasingly complex patterns, from simple contours to complete objects. Configurable hyperparameters (number of filters, stride, padding) offer adaptation flexibility according to the specific needs of each application.\r\n\r\n### Practical Applications\r\n\r\nCNNs currently find concrete applications in numerous sectors: medical diagnosis through imaging, automated surveillance systems, industrial quality control, and autonomous vehicles. Their capacity to efficiently process large quantities of visual data makes them an indispensable tool for these domains.\r\n\r\n### Technical Perspectives\r\n\r\nThe continuous optimization of CNN architectures, combined with improved computational capabilities, enables envisioning more complex applications and increased precision in image recognition tasks. Understanding these fundamental mechanisms remains essential for effectively developing and implementing these technological solutions.","src/data/blog/en/convolutional-neural-networks.mdx","88077f596d29ff3f","en/histoire-du-design-graphique",{"id":194,"data":196,"body":204,"filePath":205,"digest":206,"deferredRender":89},{"title":197,"contentType":77,"isDraft":78,"taxonomies":198,"thumbnail":110,"summary":201,"pubDate":202,"author":203},"History of Graphic Design",[199,200],{"id":40,"collection":9},{"id":46,"collection":9},"Before constituting an art of selling, graphic design was an art of uniting, dominating, sometimes intimidating.",["Date","2025-07-01T00:00:00.000Z"],{"id":64,"collection":62},"\u003Cblockquote>\r\n  Before constituting an art of selling, graphic design was an art of uniting,\r\n  dominating, sometimes intimidating.\r\n\u003C/blockquote>\r\n\r\nFrom Roman shields to medieval blazons, revolutionary flags to\r\ntwentieth-century military insignia, each symbol, color, and form\r\nconveyed a message: \"We are united,\" \"We are powerful,\" \"We are\r\ndifferent.\" These visual signs already carried the functions attributed\r\ntoday to graphic design: identity, messaging, strategy.\r\n\r\nToday, I propose a journey through the history of graphic design.\r\nConstituting the genuine foundation of all visual activity and commercial experience, it serves equally to reinforce the brand image of an entity or social collective and to orient our manner of perceiving, classifying, and judging our surroundings.\r\n\r\nIn this article, we will discover major artistic movements\r\nthat not only shaped the history of graphic design but\r\ncontinue inspiring contemporary creators.\r\n\r\n## Arts and Crafts\r\n\r\n\u003Cimg\r\n  src=\"/images/arts-craft-movement-william-morris-13.jpg\"\r\n  alt=\"Arts & Crafts\"\r\n/>\r\n\r\nThe Arts & Crafts movement, born in the 1860s in the\r\nUnited Kingdom, was a direct reaction against mechanization and mass production brought by the Industrial Revolution.\r\n\r\nIts objective was to rehabilitate artisanal work, material\r\nquality, handmade object beauty, and unity between\r\nthe artist, artisan, and object.\r\n\r\nToday, this current reflects in minimalism, handmade\r\nand home-based work, as well as in the utilization of local, ecological, and\r\ndurable materials.\r\n\r\n## Art Nouveau\r\n\r\nArt Nouveau is an artistic movement born at the end of the nineteenth century, continuing Arts & Crafts while sharing a similar desire to rupture with industrialization and academic reproduction of ancient styles.\r\n\r\nIt is characterized by the usage of floral motifs, organic forms, and curved lines, translating a harmony research between humans, nature, and art. This style aimed to integrate art into daily life, rendering beautiful and expressive the most utilitarian objects. In collective imagination, Art Nouveau embodies the momentum of modernity and the flourishing of a new human, at the dawn of the twentieth century.\r\n\r\n\u003Cimg\r\n  src=\"/images/Tile_panel_flowers_Louvre_OA3919-2-297.jpg\"\r\n  alt=\"Art Nouveau 1\"\r\n/>\r\n\u003Cimg src=\"/images/Art_Nouveau_composition.jpg\" alt=\"Art Nouveau 2\" />\r\n\r\n## Russian Constructivism\r\n\r\n\u003Cimg\r\n  src=\"/images/Alexander-Rodchenko-Books-1924.-Image-via-analogue76.com_.webp\"\r\n  alt=\"Russian Constructivism\"\r\n/>\r\n\r\nRussian Constructivism is an artistic movement born in\r\nthe 1910s-1920s in Russia, within the tumultuous context of the October Revolution. Unlike preceding\r\ncurrents, it rejects all forms of figurative or decorative representation, favoring strict geometric forms, clear\r\nstructure, and functional\r\naesthetics.\r\n\r\nThis movement considers the artwork as a \"constructed object,\" a sort of three-dimensional skeleton serving society. It advocates useful art, oriented toward industry, architecture,\r\npropaganda, and mass media.\r\n\r\n### Contemporary Heritage: Brutalist Web Design\r\n\r\nThe aesthetics of Russian Constructivism profoundly\r\ninfluenced what is called today brutalist design,\r\nparticularly visible on certain contemporary websites (my site).\r\n\r\nThis brutalist style, like Constructivism, rejects dominant aesthetic conventions, and privileges radical, functional, and unadorned choices:\r\n\r\n- very marked typography,\r\n- rigid blocks,\r\n- frank colors,\r\n- absence of \"classical\" information hierarchization,\r\n- and sometimes deliberately disconcerting navigation.\r\n\r\nObjective: shocking, interpellating, freeing from \"smooth web\" to\r\npropose a raw and assumed experience, almost militant.\r\n\r\n## Bauhaus\r\n\r\n\u003Cimg src=\"/images/616zajxgtRL.jpg\" alt=\"Bauhaus\" />\r\n\r\nBauhaus is an artistic movement and design school\r\nfounded in 1919 in Weimar, Germany, by architect Walter Gropius. It is often considered the foundation of modern design, at the crossroads of art, architecture, and industry.\r\n\r\n### Objective\r\n\r\nBauhaus's principal concept was reconciling art and craftsmanship in an era dominated by industrialization. Rather than opposing the two, Bauhaus sought to unify them to create objects that were simultaneously functional, aesthetic, and accessible to the greatest number.\r\n\r\n\u003Cblockquote>\r\n  \"Form follows function\" became one of their key principles.\r\n\u003C/blockquote>\r\n\r\n### Characteristics\r\n\r\n- Simple geometric forms\r\n- Primary colors\r\n- Absence of ornamentation\r\n- Priority to usage rather than decoration\r\n- Everyday object conception (furniture, posters,\r\n  buildings)\r\n\r\n### Heritage\r\n\r\nBauhaus influenced graphic design, architecture, typography, and even urban planning. Today, we find its spirit in minimalist interfaces, geometric fonts, simple logos, or even IKEA furniture.\r\n\r\n## International Style (Swiss Style)\r\n\r\n\u003Cimg src=\"/images/Dessau_bauhaus_04.jpg\" alt=\"International Style\" />\r\n\r\nThe International Style, also called Swiss Style, is a graphic current born in Switzerland in the 1950s. It inherits Bauhaus principles and distinguishes itself through a rigorous, functional, and minimalist design approach.\r\n\r\nThis style relies on grid usage, sans-serif typography (such as Helvetica), and clear visual hierarchy. The objective is transmitting information in a neutral, efficient, and readable manner, without superfluous ornamentation.\r\n\r\nThis current profoundly influenced editorial design, modern posters,\r\nsignage, and later, web design and digital interfaces.\r\n\r\n### Key Elements:\r\n\r\n- Page layout grid\r\n- Sans-serif typography (Helvetica, Univers...)\r\n- Simplicity, clarity, hierarchy\r\n- Geometric forms\r\n- Balance between text and image\r\n\r\n## Pop Art\r\n\r\n\u003Cimg src=\"/images/girl-pop-art-canvas-695502_1600x.webp\" alt=\"Pop Art\" />\r\n\r\nPop Art is recognized by its vivid colors, simple forms, and usage of popular images such as those from advertising, packaging, or comics.\r\n\r\nIt breaks codes by utilizing everyday elements to create accessible\r\nand impactful art.\r\n\r\nToday, we find its influence in graphic design, advertising, or social networks, where strong and direct images are central to communication.\r\n\r\n## Postmodernism\r\n\r\n\u003Cimg\r\n  src=\"/images/360_F_973913124_ExyhYfl8rIzVqbwKjT7CH3N9fnRzOJUA.jpg\"\r\n  alt=\"Postmodernism\"\r\n/>\r\n\r\nPostmodernism appears around the 1970s, as a reaction to modernism and its excessively strict, cold, and sometimes judged overly serious forms.\r\n\r\nIt breaks rules: mixing styles, playing with colors, revisiting the past while adding a humorous or offbeat touch. It is a current refusing to follow a single path.\r\n\r\nIn architecture, design, or graphic design, postmodernism questions the concept\r\nof \"good taste\" and valorizes freedom of expression, even if\r\nit shocks or surprises.\r\n\r\nToday, we still see traces in original, colorful, and unexpected creations mixing eras and influences.\r\n\r\n## Summary\r\n\r\n- Arts & Crafts (1860s):  \r\n   Return to handmade, reaction against industrialization.\r\n\r\n- Art Nouveau (late 19th – early 20th):  \r\n   Curved lines, nature, harmony\r\n\r\n- Russian Constructivism (1910s–1920s):  \r\n   Geometric forms, visual propaganda.\r\n\r\n- Bauhaus (1919–1933):  \r\n   Functionality, simplicity, art and industry union.\r\n\r\n- International Style (1920s–1980s):  \r\n   Modern architecture, steel, glass, refinement.\r\n\r\n- Swiss Typographic Style / Swiss Style (1950s):  \r\n   Grid, clear typography, Helvetica, major influence in graphic design.\r\n\r\n- Pop Art (1950s–1970s):  \r\n   Vivid colors, mass culture, Andy Warhol.\r\n\r\n- Postmodernism (1970s–1990s):  \r\n   Rupture with modernism, style mixing.\r\n\r\nFrom artisanal work to digital creations, graphic design has\r\nceaselessly evolved to reflect its era.  \r\nEach movement recounts a history, reveals a worldview, and still\r\ninfluences today our manner of communicating. Understanding this history\r\nhelps us better grasp current challenges and imagine design's future, always\r\nin quest of innovation and meaning.","src/data/blog/en/histoire-du-design-graphique.mdx","9b77858f799dc165","en/load-balancer",{"id":207,"data":209,"body":216,"filePath":217,"digest":218,"deferredRender":89},{"title":210,"contentType":77,"isDraft":78,"taxonomies":211,"thumbnail":149,"summary":213,"pubDate":214,"author":215},"Load Balancing",[212],{"id":35,"collection":9},"Load balancing, or load distribution, is a technique utilized to improve the availability, scalability, and resilience of modern web applications by distributing traffic among multiple servers.",["Date","2025-07-21T00:00:00.000Z"],{"id":64,"collection":62},"Daily, thousands of users connect to consult their\r\nemails, exchange messages, or work on their projects. An\r\nincalculable number of network packets transit each second between thousands of\r\nservers, all optimized to respond as rapidly as possible and\r\nefficiently distribute resources.\r\n\r\nHowever, **user multiplication** entails **simultaneous\r\nrequests**, overload risks, slowdowns, or even failures.\r\n\r\nIn this performance and reactivity race, numerous techniques\r\nare implemented to guarantee fluid and balanced load\r\ndistribution: **load balancing** constitutes one of the keys.\r\n\r\n**Load Balancing** is a key process in computing, frequently utilized\r\nin distributed systems architecture. It enables distributing a set\r\nof tasks or requests across multiple resources (such as servers) to\r\n**reduce overload**, **improve global performance**, and\r\n**guarantee high availability**. It primarily applies **at the\r\napplication protocol level** such as **HTTP/HTTPS, FTP, SMTP, DNS, SSH**,\r\netc., to efficiently manage network traffic.\r\n\r\n\u003Cimg\r\n  src=\"/load_balancer/illustration_load-balancer.png\"\r\n  alt=\"Diagram of a load balancer distributing traffic\"\r\n/>\r\n\r\nConcretely, instead of each client directly addressing a given\r\nserver, all requests are sent to a **central network address** —\r\nthat of the load balancer. This then **redirects traffic**\r\ntoward available servers according to different criteria (algorithms, current\r\nload, availability, etc.).\r\n\r\nThis mechanism prevents overloads, improves system global performance, and guarantees high service availability.\r\n\r\n## Load Balancing: Level 4 vs. Level 7\r\n\r\nSeveral load balancer types exist according to the **OSI model** level on\r\nwhich they operate:\r\n\r\n- **Level 4 (transport)**: balancing occurs based on IP addresses, TCP or UDP ports. The load balancer doesn't \"see\" request content; it merely routes traffic according to low-level rules.  \r\n   Example: **HAProxy** or **AWS Network Load Balancer**.\r\n- **Level 7 (application)**: balancing occurs by analyzing HTTP request content, such as URLs, cookies, or headers. This enables finer and more contextual distribution.  \r\n   Example: **NGINX**, **Traefik**, or **AWS Application Load Balancer**.\r\n\r\nThis choice depends on application type, customization requirements, and expected performance.\r\n\r\n## Principal Balancing Algorithms:\r\n\r\n### Round Robin DNS:\r\n\r\n\u003Cimg src=\"/load_balancer/round_robin.png\" alt=\"round_robin\" />\r\n\r\n**Round Robin DNS** is a load balancing algorithm that enables\r\ndistributing traffic among multiple servers by associating **multiple IP\r\naddresses** with a **single domain name**.\r\n\r\nUnlike other methods, this technique **requires no dedicated\r\nphysical equipment**. It relies on **authoritative DNS\r\nserver** functioning ([authoritative\r\nnameserver](https://www.cloudflare.com/learning/dns/dns-server-types/#authoritative-nameserver)).\r\n\r\nIt is easily implemented via your provider's DNS management interface:\r\n\r\n- by adding **multiple A records** (for IPv4),\r\n- or **AAAA records** (for IPv6).\r\n\r\n**Advantage**: configuration simplicity.  \r\n**Limitation**: no intelligent real load management (DNS doesn't \"see\" if a server is saturated or unavailable).\r\n\r\n## Software and Hardware Load Balancers:\r\n\r\n## -- Software:\r\n\r\n### [Nginx](https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/):\r\n\r\nnginx is open-source software functioning as a **web server**,\r\n**reverse proxy**, and **load balancer**. It is recognized for its **low\r\nmemory consumption** and **high speed**, making it a\r\npreferred choice in high-traffic environments.\r\n\r\nTo configure Nginx as a load balancer, the `upstream` directive is used in the configuration file, enabling declaration of multiple backend servers:\r\n\r\n```nginx\r\nhttp {\r\n    upstream backend_servers {\r\n        server backend1.example.com;\r\n        server backend2.example.com;\r\n        server backend3.example.com;\r\n    }\r\n\r\n    server {\r\n        listen 80;\r\n\r\n        location / {\r\n            proxy_pass http://backend_servers;\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nBy default, Nginx applies **round robin**, but also supports:\r\n\r\n- `least_conn`: toward the server with fewest active connections\r\n- `ip_hash`: to maintain session on the same server\r\n\r\n## Choosing a Load Balancing Method:\r\n\r\n[NGINX](https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/#choosing-a-load-balancing-method \"Choosing a Load Balancing Method\") supports four load balancing methods: Round Robin, Least Connections, IP Hash, and Generic Hash.\r\n\r\n> **Note:** When configuring a method other than Round Robin, place the corresponding directive ([`hash`](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#hash), [`ip_hash`](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#ip_hash), [`least_conn`](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#least_conn), [`least_time`](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#least_time), or [`random`](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#random)) above the `server` directive list in the [`upstream {}`](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#upstream) block.\r\n\r\n1- **Round Robin (Default)**\r\n\r\n```nginx\r\nupstream backend {\r\n   # no load balancing method is specified for Round Robin\r\n   server backend1.example.com;\r\n   server backend2.example.com;\r\n}\r\n```\r\n\r\n    Unlike Round Robin DNS, reverse proxies like **NGINX** enable **more intelligent** and dynamic load balancing, with consideration of active connections, client IP address, or hash key.\r\n\r\n2- **[Least\r\nConnections](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#least_conn) –**\r\nA request is sent to the server with fewest active connections. This\r\nmethod also considers [server\r\nweight](https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/#weights).\r\n\r\n```nginx\r\nupstream backend {\r\n    least_conn;\r\n    server backend1.example.com;\r\n    server backend2.example.com;\r\n}\r\n```\r\n\r\n3- **[IP\r\nHash](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#ip_hash) –**\r\nThe server to which a request is sent is determined from the\r\nclient IP address. In this case, either the first three octets of the IPv4 address\r\nor the entire IPv6 address are used to calculate the hash value.\r\nThe method guarantees that requests from the same address reach\r\nthe same server, unless unavailable.\r\n\r\n```nginx\r\nupstream backend {\r\n    ip_hash;\r\n    server backend1.example.com;\r\n    server backend2.example.com;\r\n}\r\n```\r\n\r\nIf a server must be temporarily removed from load-balancing\r\nrotation, it can be marked with\r\nthe [down](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#down) parameter.\r\nThis preserves current client IP address hashing. Requests that\r\nshould have been processed by this server are automatically sent to the next\r\nserver in the group.\r\n\r\n```nginx\r\nupstream backend {\r\n    server backend1.example.com;\r\n    server backend2.example.com;\r\n    server backend3.example.com down;\r\n}\r\n```\r\n\r\n4- **Generic [Hash](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#hash) –**\r\nThe server to which a request is sent is determined from a\r\nuser-defined key. This key can be a text string, a\r\nvariable, or a combination. For example, the key can be a paired source IP\r\naddress and port. This example uses a URI:\r\n\r\n```nginx\r\nupstream backend {\r\n    hash $request_uri consistent;\r\n    server backend1.example.com;\r\n    server backend2.example.com;\r\n}\r\n```\r\n\r\n    The optional [consistent](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#hash) parameter to the `hash` directive enables [ketama](http://www.last.fm/user/RJ/journal/2007/04/10/rz_libketama_-_a_consistent_hashing_algo_for_memcache_clients) consistent hash load balancing. Requests are uniformly distributed across all upstream servers based on user-defined hashed key value. If an upstream server is added or removed from an upstream group, only a few keys are remapped, minimizing cache misses. This is useful for load-balancing cache servers or other applications that accumulate state.\r\n\r\n### [HAProxy](https://www.haproxy.com/blog/haproxy-configuration-basics-load-balance-your-servers):\r\n\r\nHAProxy is free, open-source software offering high availability and load balancing for TCP and HTTP-based applications. It distributes incoming network traffic across multiple servers to guarantee optimal utilization and scalability.\r\n\r\n**HAProxy** and **NGINX** can both perform load balancing,\r\nbut they have **important differences** in their design,\r\nbehavior, and preferred use cases.\r\n\r\n## NGINX vs HAProxy: Usage Comparison\r\n\r\n\u003Cdiv class=\"table-container\">\r\n\r\n| Criterion                    | **NGINX**                          | **HAProxy**                            |\r\n| ---------------------------- | ---------------------------------- | -------------------------------------- |\r\n| **Primary function**         | HTTP server + reverse proxy        | Load balancer (specialized)            |\r\n| **Raw performance**          | Excellent in HTTP, good generalist | Better on high network loads           |\r\n| **Balancing level**          | L7 (HTTP) + partial L4             | L4 (TCP) + highly optimized L7 (HTTP)  |\r\n| **Native HTTPS support**     | Yes (certbot, etc.)                | Possible, but more complex             |\r\n| **Configuration**            | Simple, readable config files      | More verbose but very precise          |\r\n| **Monitoring / stats**       | Basic (status module)              | Very detailed (integrated dashboard)   |\r\n| **Frequent usage**           | Web reverse proxy, CDN, cache      | Pure load balancing, high availability |\r\n| **Memory consumption**       | Low                                | Ultra-optimized also                   |\r\n| **Hot reload / live update** | Not always without interruption    | Yes, without disturbing connections    |\r\n\r\n\u003C/div>\r\n\r\n## Hardware Load Balancer\r\n\r\n    In physical form, hardware versions are physical devices installed in specific datacenters. Although capable of handling and dispatching large traffic volumes across different networks, they offer less flexibility and their costs are quite elevated.\r\n\r\n\u003Cdiv class=\"table-container\">\r\n\r\n| Name                            | Description                                                                                                                          |\r\n| ------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |\r\n| **F5 BIG-IP**                   | Most recognized. Widely used in enterprises. Enables L4 and L7 with advanced functions (SSL offloading, application firewall, etc.). |\r\n| **Cisco ACE / ACI**             | Integrated into Cisco network solutions. Less common today, but very robust in certain data centers.                                 |\r\n| **Barracuda Load Balancer ADC** | Known for simplicity, good quality/price ratio, suitable for SMEs. Also offers security functions.                                   |\r\n\r\n\u003C/div>\r\n\r\n---\r\n\r\n> In this article, we clarified and explained some of the most utilized **load balancing** algorithms.  \r\n> However, this list is far from exhaustive: tools like **HAProxy** offer other advanced distribution methods, as well as key functionalities such as:\r\n>\r\n> - **health checks** (automatic server state verification),\r\n> - **redundancy** with load balancers in **active/passive** mode,\r\n> - **automatic recovery** in case of failure.\r\n>\r\n> Commercial solutions like **NGINX Plus** also offer extended functionalities, including fine session management, real-time metrics, or native support for specific protocols.\r\n\r\nIn summary, **load balancing** is an essential component of modern architectures, guaranteeing **performance, reliability, and scalability**. Whether implemented through software solutions like **NGINX** or **HAProxy**, or specialized hardware, it plays an orchestral conductor role between users and servers. The choice of the appropriate algorithm or solution will always depend on **usage context**, **budget**, and **technical constraints**.","src/data/blog/en/load-balancer.mdx","d876cf5028c3de1b","en/i18n-internationalisation",{"id":219,"data":221,"body":228,"filePath":229,"digest":230,"deferredRender":89},{"title":222,"contentType":77,"isDraft":78,"taxonomies":223,"thumbnail":123,"summary":225,"pubDate":226,"author":227},"Internationalization: A Native Approach",[224],{"id":57,"collection":9},"i18n/l10n architecture in React using native web standards.",["Date","2025-12-07T00:00:00.000Z"],{"id":64,"collection":62},"**I18n** and **L10n** are the respective abbreviations for **internationalization** and **localization**. This is a conventional workflow that consists of preparing and adapting a web application for multilingualism and different cultures.\r\n\r\nIn this article, I propose an architectural approach in React based on Contexts, Hooks, and centered on the browser's native **[Intl](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Intl)** API. The goal? Master the data flow and understand the approach of frameworks that offer ready-made solutions, by building our own from scratch.\r\n\r\n## API Structure\r\n\r\nThe approach relies on three fundamental pillars:\r\n\r\n1- **Global State Management**: A `TranslatorProvider` that wraps the application and distributes via the Context API the active locale, the dynamically loaded translation dictionary, and the translation function.\r\n\r\nThe `useTranslator` hook then exposes a unified API to access translations (`__`), date formats (`dateFormat`), and numeric formats (`numberFormat`).\r\n\r\n2- **Native Standardization**: Delegate the complexity of formatting (dates, numbers, plurals) to the browser's **`Intl`** API.\r\n\r\n3- **Static Tooling**: A static analysis script that automatically extracts translation keys from the source code via regex, then synchronizes the JSON files for each locale by adding new keys and removing obsolete ones. This ensures that dictionaries always remain consistent with the code.\r\n\r\n## Data Architecture\r\n\r\nMy approach to data structuring naturally turned to JSON. Given the number of native browser methods that allow parsing of this format (`JSON.parse`, `fetch`, etc.), I chose to rely on this simplicity.\r\n\r\n```json\r\n{\r\n  \"Hello\": \"Bonjour\",\r\n  \"Goodbye\": \"Au revoir\"\r\n}\r\n```\r\n\r\n### State Management (State Sharing)\r\n\r\nThe architecture relies on React's **Provider/Consumer** pattern. The `TranslatorProvider` makes the dictionary available to the Context, and components consume it via the `useTranslator` hook.\r\n\r\n```typescript\r\n// TranslatorProvider.tsx\r\nexport function TranslatorProvider({ children, lang, loader }: Props) {\r\n  const [langV, setLangV] = useState\u003CLangs>(lang);\r\n  const [translations, setTranslations] = useState\u003CRecord\u003Cstring, string>>({});\r\n\r\n  useEffect(() => {\r\n    loader(langV).then(setTranslations);\r\n  }, [langV, loader]);\r\n\r\n  const translate = useCallback(\r\n    (s: string) => translations[s] || s,\r\n    [translations]\r\n  );\r\n\r\n  return (\r\n    \u003CTranslateContext.Provider value={{ translate, langV, setLangV }}>\r\n      {children}\r\n    \u003C/TranslateContext.Provider>\r\n  );\r\n}\r\n```\r\n\r\n```typescript\r\nexport function useTranslator() {\r\n  const { translate, langV, setLangV } = useContext(TranslateContext);\r\n\r\n  const dateFormat = (date: Date | string | number | null,\r\n    options: Intl.DateTimeFormatOptions = {\r\n      year: \"numeric\",\r\n      month: \"long\",\r\n      day: \"numeric\",\r\n      weekday: \"long\",\r\n    }\r\n  ) => {...};\r\n\r\n  const styleForNumber = (options: Options): Intl.NumberFormatOptions ...\r\n\r\n  const numberFormat = ( value: number | string | null, options: Options = {\r\n      style: \"decimal\",\r\n    }\r\n  ) => {...};\r\n\r\n  return {\r\n    langV,\r\n    setLangV,\r\n    dateFormat: dateFormat,\r\n    numberFormat,\r\n    __: translate,\r\n  };\r\n}\r\n```\r\n\r\n**Why this approach?**\r\nIt separates the `loader` loading from the `Provider` distribution. This makes the system agnostic: translations can come from a local JSON file, a REST API, or a headless CMS.\r\n\r\n## Abstraction around Intl\r\n\r\nThe browser's Intl API covers most internationalization needs for a web application. It contains all the world's linguistic and typographic rules: date formats by country, currency symbols, pluralization rules, etc.\r\n\r\nA **lightweight facade** is sufficient to adapt this API to our needs. The idea is to encapsulate the verbosity of `Intl` (constructors, multiple options) behind simple functions (`dateFormat`, `numberFormat`) that automatically respect the active locale.\r\n\r\n```typescript\r\n// useTranslator.ts\r\nconst styleForNumber = (options: Options): Intl.NumberFormatOptions => {\r\n  switch (options.style) {\r\n    case \"currency\":\r\n      return {\r\n        style: \"currency\",\r\n        // Currency is dynamically deduced from the locale (e.g., FR -> EUR)\r\n        currency: CurrencyCode[langV.toUpperCase()],\r\n      };\r\n    case \"unit\":\r\n      return {\r\n        style: \"unit\",\r\n        unit: options.unit, // e.g., \"kilometer\"\r\n      };\r\n    // ...\r\n  }\r\n};\r\n```\r\n\r\nThis function acts as a **normalizer** that ensures compliance with typographic conventions according to the active locale: non-breaking space, currency symbol position, decimal separator (comma or period).\r\n\r\n## Maintenance Automation\r\n\r\nFor maintenance and developer experience reasons, I implemented a script that analyzes the source code and automatically synchronizes translation files.\r\n\r\nThe tool scans all TypeScript/React files to detect calls to `__()` via regex, then updates the dictionaries for each locale.\r\n\r\n```typescript\r\n// scripts/collect.ts\r\nfunction collectStrings() {\r\n  const files = globSync(sourcePath);\r\n  const strings = new Set\u003Cstring>();\r\n\r\n  for (const file of files) {\r\n    const content = readFileSync(file, \"utf8\");\r\n    const found = extractStringsFromContent(content);\r\n    found.forEach((s) => strings.add(s));\r\n  }\r\n  return strings;\r\n}\r\n\r\n// Synchronization: adding new keys, removing obsolete ones\r\nfor (const newKey of keys.difference(existingKeys)) {\r\n  dict.set(newKey, \"\");\r\n}\r\n```\r\n\r\nA simple command (`node scripts/collect.ts` or via your task runner) ensures that JSON files remain perfectly aligned with the code.\r\n\r\n## Concrete Application\r\n\r\nThe interface below reacts to locale changes:\r\n\r\n- Adapted date formats\r\n- Localized currencies (€ vs $)\r\n- Regional units (km, °C)\r\n\r\n\u003Ciframe\r\n  src=\"https://elyseemb.github.io/i18Tp/\"\r\n  width=\"100%\"\r\n  height=\"800px\"\r\n  style=\"border-radius: 18px;\"\r\n  title=\"Native i18n system demonstration\"\r\n  loading=\"lazy\"\r\n>\u003C/iframe>\r\n\r\n## Conclusion\r\n\r\nThis architecture illustrates that modern browsers already provide many powerful abstractions. These standards are optimized and maintained by browser vendors.\r\n\r\nThe complete implementation (Provider, Hook, extraction script) is available on [GitHub](https://github.com/elyseeMB/i18Tp) for experimentation and adaptation to your projects.","src/data/blog/en/i18n-(internationalisation).mdx","a51469e833dd994d","en/lazy-loading",{"id":231,"data":233,"body":239,"filePath":240,"digest":241,"deferredRender":89},{"title":133,"contentType":77,"isDraft":78,"taxonomies":234,"thumbnail":136,"summary":236,"pubDate":237,"author":238},[235],{"id":57,"collection":9},"In production environments, optimizing the loading process and deployment of new versions in Single Page Applications (SPAs) constitutes a critical consideration for system reliability and user experience.",["Date","2025-09-14T00:00:00.000Z"],{"id":64,"collection":62},"In production environments, optimizing the loading process and deployment of new versions in Single Page Applications (SPAs) constitutes a critical consideration for system reliability and user experience. It is frequently observed that users maintaining active sessions for extended periods retain obsolete application versions at the moment of deployment. JavaScript files remain cached in the browser or active during the session, generating loading errors when users attempt to access new functionalities.\r\n\r\nIn this article, we will analyze the implementation of lazy loading within navigation and routing contexts. We will examine how this approach enhances performance while effectively managing loading errors in production environments.\r\n\r\n## Definition\r\n\r\nLazy loading represents an optimization technique utilized in programming, particularly in web development, aimed at improving performance and reducing initial loading times. This approach consists of deferring the loading of resources (images, JavaScript modules, components, etc.) until they are genuinely necessary or requested by the user.\r\n\r\n## Lazy Loading in React\r\n\r\n### The Standard Approach\r\n\r\nMost modern frameworks (React, Preact, etc.) provide native lazy loading implementations. React exposes this functionality via `React.lazy()`, however, this basic approach reveals its limitations in production environments, particularly during JavaScript chunk loading failures.\r\n\r\n### Operational Mechanism\r\n\r\nLazy loading functions by awaiting a promise resolution to a module containing a default export. When the component is requested, React extracts the `module.default` and utilizes it for rendering operations.\r\n\r\n### Basic Code Example\r\n\r\n```js\r\nimport { lazy } from \"react\";\r\nconst MarkdownPreview = lazy(() => import(\"./MarkdownPreview.js\"));\r\n```\r\n\r\n## Production Chunk Management Challenges\r\n\r\nAs mentioned in the introduction, frequent production deployments generate a recurring problem: users with active sessions retain references to legacy chunks that no longer exist on the server. When these users attempt to navigate to new application sections, loading fails because JavaScript files have been renamed with new hashes during deployment.\r\n\r\n### Chunk Definition:\r\n\r\nA chunk constitutes a segment of JavaScript code generated and optimized for production by a bundler (Vite, Webpack) during the build process.\r\n\r\n## Enhanced Methodological Approach\r\n\r\n### Reload Functionality\r\n\r\nWe begin with the essential component: chunk management to minimize loading failures attributable to obsolete hashes from previous versions.\r\n\r\n```js\r\nimport { lazy as reactLazy, type ComponentType, type FC } from \"react\";\r\n\r\nexport function createLazy(\r\n  importFunction: () => Promise\u003C{ default: ComponentType\u003Cany> }>,\r\n  maxRetries = 3\r\n) {\r\n  /**\r\n   * 1 - Automatic reload management\r\n   */\r\n  return reactLazy(async () => {\r\n    /**\r\n     * Utilizing the function signature as a sessionStorage key.\r\n     * Converting it to a string to create a unique identifier.\r\n     */\r\n    const functionString = importFunction.toString();\r\n\r\n    try {\r\n      /**\r\n       * Executing the function that imports our component\r\n       */\r\n      const component = await importFunction();\r\n\r\n      /**\r\n       * Upon success, removing the sessionStorage key beforehand\r\n       * for clean reinitialization during the next potential failure\r\n       */\r\n      sessionStorage.removeItem(functionString);\r\n\r\n      return component;\r\n    } catch (error) {\r\n      /**\r\n       * Retrieving the value stored in sessionStorage with the \"functionString\" key.\r\n       * Subsequently comparing it to our maxRetries parameter.\r\n       * If it is less than maxRetries, we increment it until it becomes greater.\r\n       * Then we refresh the page with window.location.reload().\r\n       */\r\n      const currentFailures = parseInt(\r\n        sessionStorage.getItem(functionString) || \"0\"\r\n      );\r\n\r\n      if (currentFailures \u003C maxRetries) {\r\n        sessionStorage.setItem(\r\n          functionString,\r\n          (currentFailures + 1).toString()\r\n        );\r\n        window.location.reload();\r\n\r\n        /**\r\n         * React.lazy() expects a Promise that resolves to { default: Component }.\r\n         * This is a placeholder that will never be displayed because the page reloads.\r\n         * We return a signature similar to the React.lazy() API.\r\n         */\r\n        const EmptyComponent: FC = () => null;\r\n        return { default: EmptyComponent };\r\n      }\r\n\r\n      /**\r\n       * If the maximum number of attempts has been exceeded, we propagate the error\r\n       */\r\n      throw error;\r\n    }\r\n  });\r\n}\r\n```\r\n\r\nThis approach enables automatic error handling for chunk-related failures and significantly improves user experience by preventing application crashes.\r\n\r\n### Retry Logic: Optimization\r\n\r\nSubsequently, to ensure optimal user experience, we incorporate a retry parameter that defines the number of attempts before triggering reload functionality. Thus, we ensure that temporary errors (unstable connection, momentarily unavailable server) are resolved without resorting to page reloading, which remains our last-resort solution for genuinely obsolete chunks.\r\n\r\n```js\r\nconst tryImport = async (): Promise\u003C{ default: ComponentType\u003Cany> }> => {\r\n  /**\r\n   * Initializing the counter\r\n   */\r\n  let retryCount: number = 0;\r\n\r\n  /**\r\n   * Internal function to manage import retry operations.\r\n   * We encapsulate attempt so that retryCount is local to each call\r\n   * and to avoid sharing state between multiple imports.\r\n   * Thus, we have a local function that manages its own state and manipulations.\r\n   */\r\n  const attempt = async (): Promise\u003C{ default: ComponentType\u003Cany> }> => {\r\n    try {\r\n      return await importFunction();\r\n    } catch (error) {\r\n      /**\r\n       * If the number of retries is less than the parameter argument (which was 3),\r\n       * we increment and re-execute the function (recursion)\r\n       */\r\n      if (retryCount \u003C importRetries) {\r\n        retryCount++;\r\n        /**\r\n         * We implement a pause for each retry\r\n         */\r\n        if (retryDelay > 0) {\r\n          await new Promise((resolve) => setTimeout(resolve, retryDelay));\r\n        }\r\n\r\n        return attempt();\r\n      }\r\n      /**\r\n       * In case of repeated failure, we stop and return the error\r\n       */\r\n      throw error;\r\n    }\r\n  };\r\n\r\n  return attempt();\r\n};\r\n```\r\n\r\nEncapsulating `attempt` maintains the retry counter local to each loading attempt, preventing conflicts or unexpected reinitializations in a React environment where lazy loading may be evaluated multiple times.\r\n\r\n### Complete Implementation:\r\n\r\n```js\r\nimport { lazy as reactLazy, type ComponentType, type FC } from \"react\";\r\n\r\ntype Options = {\r\n  maxRetries: number,\r\n  importRetries: number,\r\n  retryDelay: number,\r\n};\r\n\r\nexport function createLazy(\r\n  importFunction: () => Promise\u003C{ default: ComponentType\u003Cany> }>,\r\n  { maxRetries, importRetries, retryDelay }: Options = {\r\n    maxRetries: 3,\r\n    importRetries: 3,\r\n    retryDelay: 300,\r\n  }\r\n) {\r\n  /**\r\n   * Retry feature\r\n   *\r\n   * Creating a function with a signature similar to what createLazy() should return\r\n   */\r\n  const tryImport = async (): Promise\u003C{ default: ComponentType\u003Cany> }> => {\r\n    /**\r\n     * Initializing the counter\r\n     */\r\n    let retryCount: number = 0;\r\n\r\n    /**\r\n     * Internal function to manage import retry operations.\r\n     * We encapsulate attempt so that retryCount is local to each call\r\n     * and to avoid sharing state between multiple imports.\r\n     * Thus, we have a local function that manages its own state and manipulations.\r\n     */\r\n    const attempt = async (): Promise\u003C{ default: ComponentType\u003Cany> }> => {\r\n      try {\r\n        return await importFunction();\r\n      } catch (error) {\r\n        /**\r\n         * If the number of retries is less than the parameter argument (which was 3),\r\n         * we increment and re-execute the function (recursion)\r\n         */\r\n        if (retryCount \u003C importRetries) {\r\n          retryCount++;\r\n          /**\r\n           * We implement a pause for each retry\r\n           */\r\n          if (retryDelay > 0) {\r\n            await new Promise((resolve) => setTimeout(resolve, retryDelay));\r\n          }\r\n\r\n          return attempt();\r\n        }\r\n        /**\r\n         * In case of repeated failure, we stop and return the error\r\n         */\r\n        throw error;\r\n      }\r\n    };\r\n\r\n    return attempt();\r\n  };\r\n\r\n  /**\r\n   * Automatic reload management\r\n   */\r\n  return reactLazy(async () => {\r\n    /**\r\n     * Utilizing the function signature as a sessionStorage key.\r\n     * Converting it to a string to create a unique identifier.\r\n     */\r\n    const functionString = importFunction.toString();\r\n\r\n    try {\r\n      /**\r\n       * Here we execute the tryImport function which imports our component\r\n       * and performs preliminary processing before executing a reload\r\n       */\r\n      const component = await tryImport();\r\n\r\n      /**\r\n       * Upon success, removing the sessionStorage key beforehand\r\n       * for clean reinitialization during the next potential failure\r\n       */\r\n      sessionStorage.removeItem(functionString);\r\n\r\n      return component;\r\n    } catch (error) {\r\n      /**\r\n       * Retrieving the value stored in sessionStorage with the \"functionString\" key.\r\n       * Subsequently comparing it to our maxRetries parameter.\r\n       * If it is less than maxRetries, we increment it until it becomes greater.\r\n       * Then we refresh the page with window.location.reload().\r\n       */\r\n      const currentFailures = parseInt(\r\n        sessionStorage.getItem(functionString) || \"0\"\r\n      );\r\n\r\n      if (currentFailures \u003C maxRetries) {\r\n        sessionStorage.setItem(\r\n          functionString,\r\n          (currentFailures + 1).toString()\r\n        );\r\n\r\n        window.location.reload();\r\n\r\n        /**\r\n         * React.lazy() expects a Promise that resolves to { default: Component }.\r\n         * This is a placeholder that will never be displayed because the page reloads.\r\n         * We return a signature similar to the React.lazy() API.\r\n         */\r\n        const EmptyComponent: FC = () => null;\r\n\r\n        return { default: EmptyComponent };\r\n      }\r\n      /**\r\n       * If the maximum number of attempts has been exceeded, we propagate the error\r\n       */\r\n      throw error;\r\n    }\r\n  });\r\n}\r\n```\r\n\r\n## Solutions Provided by This Approach\r\n\r\nThis `createLazy` implementation provides several significant improvements over standard lazy loading:\r\n\r\n1. **Robust Error Handling and Retry Mechanisms**  \r\n   The component is imported with a configurable retry system (`importRetries`) and a delay between attempts (`retryDelay`). This enables better management of network issues or temporary failures during component loading, thereby reducing application crash risks.\r\n\r\n2. **Efficient Chunk Management**  \r\n   By controlling lazy loading at the import level and utilizing unique keys for each imported function, this approach facilitates JavaScript chunk management for production environments. This optimizes resource delivery and simplifies **continuous deployment**, as problematic components can be automatically reloaded without interrupting user experience.\r\n\r\n3. **User Experience Enhancement Potential**  \r\n   While this version focuses on robustness and reliability, the mechanism can be extended to include loading state tracking. For example: displaying a **skeleton loader** or progress indicator while the component is loading. This would directly improve user experience while maintaining the benefits of retry and automatic reload functionality.\r\n\r\n## Simulation\r\n\r\nTo demonstrate the functionality of this code, certain lazy-loaded components were deliberately configured to fail. Each component attempts to reload multiple times with delays between attempts, and if it continues to fail, the page can automatically reload. The objective is not to achieve a fluid interface, but to concretely demonstrate how the retry and reload mechanism operates.\r\n\r\n\u003Ciframe\r\n  src=\"https://elyseemb.github.io/lazyLoading/\"\r\n  style=\"width:100%; height:500px; border:0; border-radius:8px;\"\r\n>\u003C/iframe>\r\n\r\nGitHub Repository: [https://github.com/elyseeMB/lazyLoading.git](https://github.com/elyseeMB/lazyLoading.git)\r\n\r\n## Conclusion\r\n\r\nIn summary, this approach enables more reliable and resilient lazy loading with tangible benefits for production and maintenance. It effectively resolves the critical problem of obsolete chunks that daily affects SPA users.\r\n\r\nThis implementation constitutes a solid foundation adaptable to your specific needs: monitoring, user feedback, environment-specific configuration. The principal advantage resides in its adoption simplicity—a few lines suffice to transform fragile lazy loading into a robust system.","src/data/blog/en/lazy-loading.mdx","1fe19c76a1deb8aa","en/qu-est-ce-que-design-pattern",{"id":242,"data":244,"body":252,"filePath":253,"digest":254,"deferredRender":89},{"title":245,"contentType":77,"isDraft":78,"taxonomies":246,"thumbnail":163,"summary":249,"pubDate":250,"author":251},"What is a Design Pattern?",[247,248],{"id":17,"collection":9},{"id":57,"collection":9},"Design patterns are proven and validated solutions established by computing pioneers. They were conceived to structure and organize code in clear, readable, and efficient manners.",["Date","2025-07-12T00:00:00.000Z"],{"id":64,"collection":62},"import BlockDesignPattern from \"../../../components/blog/pattern/BlockDesignPattern.tsx\";\r\nimport DisplayList from \"../../../components/blog/pattern/DisplayList.tsx\";\r\n\r\n**Design patterns** are proven and validated solutions established by\r\ncomputing pioneers. They were conceived to **structure and organize\r\ncode** in clear, readable, and efficient manners. The concept proposes\r\n**generic and reusable models** that facilitate **scalability** and\r\n**maintenance** of software systems while reducing common\r\nerrors.\r\n\r\n## History\r\n\r\n**Design patterns** emerged from the desire to produce\r\n**scalable** and maintainable code, even if it initially meant **sacrificing\r\nsome development comfort**. When technical teams\r\nobserve that identical structures or solutions are **repeated multiple times in\r\ncode** to resolve similar problem types, they eventually\r\nassign a **common name**. Thus the concept of **design\r\npatterns** was born: **named, generic, and reusable** solutions,\r\ndocumented to facilitate collaboration and comprehension in\r\nsoftware projects.\r\n\r\n\u003Cblockquote>\r\n  \u003Cem>\r\n    Historically, the design pattern concept was inspired by the work of\r\n    **Christopher Alexander** in architecture, subsequently formalized in\r\n    computing by the **\"Gang of Four\"** in 1994. These patterns became an\r\n    indispensable foundation for structuring code, avoiding redundancies, and\r\n    facilitating large-scale application maintenance.\r\n  \u003C/em>\r\n\u003C/blockquote>\r\n\r\n## Why Should I Learn Patterns?\r\n\r\nIn the majority of cases, you have probably already **utilized design\r\npatterns unknowingly**. This precisely distinguishes\r\njunior developers from senior developers: the capacity to recognize,\r\ncomprehend, and consciously apply these structures. For example, **framework\r\nsource code** (such as React, Laravel, AdonisJS, etc.) is replete with them.\r\n\r\nOnce mastered, **design patterns** become a genuine **toolkit**\r\nfor efficiently resolving common development problems.\r\nThey will help you **think differently**, with more structure and perspective.\r\nOver time, this manner of reasoning becomes almost natural.\r\n\r\n## Classifications and Roles:\r\n\r\nThis list is non-exhaustive and does not cover all design\r\npatterns you might encounter. We will concentrate on a\r\nparticular model type: **architectural patterns**.\r\n\r\nThese models can be implemented in **any programming\r\nlanguage** and are called **universal patterns** or **high-level\r\npatterns**. They contrast with **idioms**, which are more\r\nspecific solutions, language-particular and often low-level.\r\n\r\n\u003Cblockquote>\r\n- **Architectural patterns** define the global structure of an\r\n  application or system (e.g., MVC, Client-Server, Microservices).\r\n\r\n- **Design patterns** (in the more classical sense) concern\r\n  code organization within components (e.g., Singleton, Factory,\r\n  Observer).\r\n\r\n- **Idioms** are language-specific constructions that\r\n  exploit syntactic and semantic specificities.\r\n\r\n \u003C/blockquote>\r\n\r\n\u003CDisplayList client:only />\r\n\r\n---\r\n\r\n## Concrete Case: A Zustand Store Refactored According to Design Patterns\r\n\r\nTo concretely illustrate design pattern benefits, consider a\r\n**real example** encountered during application construction.\r\nI utilize Zustand as a **global state management** solution. It's a\r\nsimple yet powerful tool enabling on-the-fly store construction.\r\n\r\nHowever, advancing in development, the store becomes **complex,\r\ndifficult to test or maintain**, and certain logics are duplicated.\r\nThis is exactly the type of situation where **design patterns make complete\r\nsense**.\r\n\r\nIn this section, we will **refactor this store step-by-step**,\r\napplying different design patterns. Each refactor will be **associated with a\r\nspecific pattern**: Singleton, Factory, Observer, etc. This will enable\r\nunderstanding **both the theory** behind each model **and its\r\npractical application** in a modern context (React + Zustand).\r\n\r\n## Base Code\r\n\r\nBefore beginning refactoring, here is **the Zustand store as it initially\r\nexisted** in my project. It is functional and partially implemented\r\ncertain patterns\r\n\r\n- Factory Method (partially)\r\n- Strategy\r\n\r\n```ts\r\nimport { UnAuthenticatedError } from \"@helpers/website\";\r\n\r\nimport {\r\n  createContext,\r\n  useContext,\r\n  useMemo,\r\n  type PropsWithChildren,\r\n} from \"react\";\r\n\r\nimport { create, useStore as useZustandStore } from \"zustand\";\r\n\r\nimport { combine, persist } from \"zustand/middleware\";\r\n\r\nimport type { Account } from \"./hooks/useAuth.ts\";\r\n\r\nimport type {\r\n  AccessLevels,\r\n  Courses,\r\n  Difficulties,\r\n  Statuses,\r\n} from \"@api/website/types\";\r\n\r\nexport type ResourceMap = {\r\n  accessLevel: AccessLevels;\r\n\r\n  difficulties: Difficulties;\r\n\r\n  statuses: Statuses;\r\n};\r\n\r\ntype State = {\r\n  account: undefined | null | Record\u003Cstring, any>;\r\n\r\n  organization: Record\u003Cstring, any>;\r\n\r\n  accesslevels: AccessLevels[];\r\n\r\n  difficulties: Difficulties[];\r\n\r\n  statuses: Statuses[];\r\n\r\n  courses: Courses[];\r\n};\r\n\r\nfunction getStateKey\u003CT extends keyof ResourceMap>(\r\n  type: T\r\n): keyof Omit\u003CState, \"account\" | \"organization\" | \"courses\"> {\r\n  switch (type) {\r\n    case \"accessLevel\":\r\n      return \"accesslevels\";\r\n\r\n    case \"difficulties\":\r\n      return \"difficulties\";\r\n\r\n    case \"statuses\":\r\n      return \"statuses\";\r\n\r\n    default:\r\n      throw new Error(\"Courses resource type \" + type);\r\n  }\r\n}\r\n\r\nconst createStore = () =>\r\n  create(\r\n    persist(\r\n      combine(\r\n        {\r\n          account: undefined as undefined | null | Account,\r\n\r\n          organization: {},\r\n\r\n          courses: [],\r\n\r\n          accesslevels: [],\r\n\r\n          difficulties: [],\r\n\r\n          statuses: [],\r\n        } as State,\r\n\r\n        (set) => ({\r\n          setResources: function \u003CT extends keyof ResourceMap>(\r\n            type: T,\r\n\r\n            data: ResourceMap[T][]\r\n          ) {\r\n            const key = getStateKey(type);\r\n\r\n            return set({ [key]: data });\r\n          },\r\n\r\n          addResource: function \u003CT extends keyof ResourceMap>(\r\n            type: T,\r\n\r\n            newData: ResourceMap[T]\r\n          ) {\r\n            const key = getStateKey(type);\r\n\r\n            return set((state) => ({\r\n              [key]: [...state[key], newData],\r\n            }));\r\n          },\r\n\r\n          updateResource: function \u003CT extends keyof ResourceMap>(\r\n            type: T,\r\n\r\n            newData: ResourceMap[T]\r\n          ) {\r\n            const key = getStateKey(type);\r\n\r\n            return set((state) => ({\r\n              [key]: state[key].map((item) =>\r\n                item.id === newData.id ? { ...item, ...newData } : item\r\n              ),\r\n            }));\r\n          },\r\n\r\n          deleteResource: function \u003CT extends keyof ResourceMap>(\r\n            type: T,\r\n\r\n            id: number\r\n          ) {\r\n            const key = getStateKey(type);\r\n\r\n            return set((state) => ({\r\n              [key]: state[key].filter((item) => item.id !== id),\r\n            }));\r\n          },\r\n\r\n          setCourses: (courses: Courses[]) => {\r\n            set({ courses });\r\n          },\r\n\r\n          addCourse: (course: Courses) => {\r\n            set((state) => ({\r\n              courses: [...state.courses, course],\r\n            }));\r\n          },\r\n\r\n          updateOrganization: (newDate: Record\u003Cstring, any>) =>\r\n            set({ organization: newDate }),\r\n\r\n          updateAccount: (account: Account | null) => set({ account }),\r\n        })\r\n      ),\r\n\r\n      {\r\n        name: \"account\",\r\n      }\r\n    )\r\n  );\r\n\r\ntype Store = ReturnType\u003Ctypeof createStore>;\r\n\r\ntype StoreState = Store extends {\r\n  getState: () => infer T;\r\n}\r\n  ? T\r\n  : never;\r\n\r\nconst StoreContext = createContext\u003C{ store?: Store }>({});\r\n\r\nexport function StoreProvider({ children }: PropsWithChildren) {\r\n  const store = useMemo(() => createStore(), []);\r\n\r\n  return (\r\n    \u003CStoreContext.Provider value={{ store: store }}>\r\n      {children}{\" \"}\r\n    \u003C/StoreContext.Provider>\r\n  );\r\n}\r\n\r\nexport function useStore\u003CT>(selector: (state: StoreState) => T) {\r\n  const store = useContext(StoreContext).store;\r\n\r\n  if (!store) {\r\n    throw new Error(\"A context need to be provider to use the store\");\r\n  }\r\n\r\n  return useZustandStore(store, selector);\r\n}\r\n\r\nexport type InferResourceType\u003CT> = T extends keyof ResourceMap\r\n  ? ResourceMap[T]\r\n  : never;\r\n\r\nexport function useResource\u003CT extends keyof ResourceMap>(type: T) {\r\n  const key = getStateKey(type);\r\n\r\n  const list = useStore((state) => state[key]) as InferResourceType\u003CT>[];\r\n\r\n  const setResources = useStore((state) => state.setResources);\r\n\r\n  const addResource = useStore((state) => state.addResource);\r\n\r\n  const updateResource = useStore((state) => state.updateResource);\r\n\r\n  const deleteResource = useStore((state) => state.deleteResource);\r\n\r\n  return {\r\n    list,\r\n\r\n    set: (data: InferResourceType\u003CT>[]) => setResources(type, data),\r\n\r\n    add: (data: InferResourceType\u003CT>) => addResource(type, data),\r\n\r\n    update: (data: InferResourceType\u003CT>) => updateResource(type, data),\r\n\r\n    delete: (id: number) => deleteResource(type, id),\r\n  };\r\n}\r\n\r\n// ACCESS_LEVELS\r\n\r\nexport function useAccessLevels() {\r\n  return useResource(\"accessLevel\");\r\n}\r\n\r\n// DIFFICULTIES\r\n\r\nexport function useDifficulties() {\r\n  return useResource(\"difficulties\");\r\n}\r\n\r\n// STATUSES\r\n\r\nexport function useStatuses() {\r\n  return useResource(\"statuses\");\r\n}\r\n\r\n// COURSES\r\n\r\nexport function useCourses() {\r\n  const list = useStore((state) => state.courses);\r\n\r\n  const setCourses = useStore((state) => state.setCourses);\r\n\r\n  const addCourses = useStore((state) => state.addCourse);\r\n\r\n  return {\r\n    list,\r\n\r\n    set: (data: Courses[]) => setCourses(data),\r\n\r\n    add: (data: Courses) => addCourses(data),\r\n  };\r\n}\r\n\r\n// ORGANISATION\r\n\r\nexport function useOrganization() {\r\n  return useStore((state) => state.organization);\r\n}\r\n\r\nexport function useUpdateOrganization() {\r\n  return useStore((state) => state.updateOrganization);\r\n}\r\n\r\nexport function useUpdateAccount() {\r\n  return useStore((state) => state.updateAccount);\r\n}\r\n\r\nexport function useIsAuth() {\r\n  const account = useStore((state) => state.account);\r\n\r\n  if (!account) {\r\n    throw new UnAuthenticatedError();\r\n  }\r\n\r\n  return {\r\n    ...account,\r\n  };\r\n}\r\n\r\nexport function useAccount() {\r\n  const account = useStore((state) => state.account);\r\n\r\n  return {\r\n    ...account,\r\n  };\r\n}\r\n```\r\n\r\n### Refactored Version Objectives\r\n\r\n- Separate responsibilities.\r\n- Apply classical patterns.\r\n- Maintain clean and extensible API.\r\n\r\n## Singleton:\r\n\r\nA singleton ensures that only one instance of an object (preferably\r\na class) is initialized, thus offering a single global\r\ninitialization point. In our situation, we are in JavaScript where each\r\nobject and module is unique in its execution context.\r\n\r\n— **Singleton Pattern**\r\n\r\n```ts\r\nimport { create } from \"zustand\";\r\nimport { combine, persist } from \"zustand/middleware\";\r\nimport type { State, Store, ResourceKey, InferResourceType } from \"./types\";\r\nimport { getStateKey } from \"./factory\";\r\n\r\nlet storeInstance: Store | undefined;\r\n\r\nexport const createStore = (): Store => {\r\n  if (storeInstance) return storeInstance;\r\n\r\n  storeInstance = create(\r\n    persist(\r\n      combine(\r\n        {\r\n          account: undefined,\r\n          organization: {},\r\n          courses: [],\r\n          accesslevels: [],\r\n          difficulties: [],\r\n          statuses: [],\r\n        } as State,\r\n        (set) => ({\r\n          updateAccount: (account) => set({ account }),\r\n          updateOrganization: (org) => set({ organization: org }),\r\n\r\n          setResources: \u003CT extends ResourceKey>(\r\n            type: T,\r\n            data: InferResourceType\u003CT>[]\r\n          ) => set({ [getStateKey(type)]: data }),\r\n\r\n          addResource: \u003CT extends ResourceKey>(\r\n            type: T,\r\n            item: InferResourceType\u003CT>\r\n          ) =>\r\n            set((state) => ({\r\n              [getStateKey(type)]: [...state[getStateKey(type)], item],\r\n            })),\r\n\r\n          updateResource: \u003CT extends ResourceKey>(\r\n            type: T,\r\n            item: InferResourceType\u003CT>\r\n          ) =>\r\n            set((state) => ({\r\n              [getStateKey(type)]: state[getStateKey(type)].map((i) =>\r\n                i.id === item.id ? { ...i, ...item } : i\r\n              ),\r\n            })),\r\n\r\n          deleteResource: \u003CT extends ResourceKey>(type: T, id: number) =>\r\n            set((state) => ({\r\n              [getStateKey(type)]: state[getStateKey(type)].filter(\r\n                (i) => i.id !== id\r\n              ),\r\n            })),\r\n        })\r\n      ),\r\n      { name: \"account\" }\r\n    )\r\n  );\r\n\r\n  return storeInstance;\r\n};\r\n```\r\n\r\nThis ensures a single Zustand store exists in the application, which is important\r\nto avoid inconsistencies or unnecessary re-renders in React `createStore()`\r\nin the React context.\r\n\r\nI will not revisit Zustand usage in detail here; that will be addressed in a forthcoming article.\r\nIn brief:\r\n\r\n- **Combine**: is a middleware that enables state and action separation.\r\n- **Persist**: is a middleware that enables persistence with localStorage.\r\n\r\n## Factory\r\n\r\n— **Factory Pattern for Keys**\r\n\r\n```ts\r\nexport const getStateKey = \u003CT extends ResourceKey>(type: T): keyof State => {\r\n  const map: Record\u003CResourceKey, keyof State> = {\r\n    accessLevel: \"accesslevels\",\r\n    difficulties: \"difficulties\",\r\n    statuses: \"statuses\",\r\n  };\r\n  const key = map[type];\r\n  if (!key) throw new Error(`Unknown resource type: ${type}`);\r\n  return key;\r\n};\r\n```\r\n\r\nWe **abstract the logic** of mapping `\"accessLevel\"` → `\"accesslevels\"` into\r\na **declarative** object, instead of a `switch` statement.\r\n\r\n## Facade\r\n\r\n— **Facade Pattern**\r\n\r\n```ts\r\nexport const useAccount = () => {\r\n  const account = useStore((s) => s.account);\r\n  return { ...account };\r\n};\r\n```\r\n\r\nWe hide store complexity and expose a simple API.\r\n\r\n## Illustrations in Pseudo-code\r\n\r\n\u003Cblockquote>\r\n  Although this article aims to provide **concrete implementation** of design\r\n  patterns in a real context (React + Zustand), certain models like\r\n  **Singleton** or **Factory Method** integrate naturally into my store\r\n  architecture. However, other models like **Builder**, **Strategy**, or\r\n  **Decorator** are more **conceptual** in this context. They will therefore be\r\n  illustrated more **generically in pseudo-code** to facilitate comprehension.\r\n  These examples are **not intended for direct copying** into Zustand or React\r\n  projects, but rather to help you **grasp the general concept** behind each\r\n  pattern. You will subsequently see how to **adapt these concepts** in a real\r\n  project if necessary.\r\n\u003C/blockquote>\r\n\r\n### Builder (constructing objects step-by-step)\r\n\r\n```ts\r\nclass CourseBuilder {\r\n  name = \"\";\r\n  color = \"\";\r\n\r\n  setName(name: string) {\r\n    this.name = name;\r\n    return this;\r\n  }\r\n\r\n  setColor(color: string) {\r\n    this.color = color;\r\n    return this;\r\n  }\r\n\r\n  build() {\r\n    return { name: this.name, color: this.color };\r\n  }\r\n}\r\n\r\nconst course = new CourseBuilder().setName(\"React\").setColor(\"blue\").build();\r\n```\r\n\r\n### Strategy (dynamically changing behavior)\r\n\r\n```ts\r\nclass ExportStrategy {\r\n  execute(data) {\r\n    throw \"Not implemented\";\r\n  }\r\n}\r\n\r\nclass JsonExport extends ExportStrategy {\r\n  execute(data) {\r\n    return JSON.stringify(data);\r\n  }\r\n}\r\n\r\nclass CsvExport extends ExportStrategy {\r\n  execute(data) {\r\n    return data.map((row) => row.join(\",\")).join(\"\\n\");\r\n  }\r\n}\r\n\r\nfunction exportData(data, strategy: ExportStrategy) {\r\n  return strategy.execute(data);\r\n}\r\n```\r\n\r\n### Decorator (enriching behavior without touching source code)\r\n\r\n```ts\r\nfunction withLogger(fn) {\r\n  return function (...args) {\r\n    return fn(...args);\r\n  };\r\n}\r\n\r\nfunction saveCourse(course) {}\r\n\r\nconst loggedSaveCourse = withLogger(saveCourse);\r\n\r\nloggedSaveCourse({ name: \"JS\", color: \"yellow\" });\r\n```\r\n\r\n\u003Chr />\r\n\r\n## Conclusion\r\n\r\n**Design patterns** are powerful tools, provided they are utilized\r\nin the **appropriate context** and thoughtfully. They can be considered **upstream**,\r\nduring conception, if comfortable, or **progressively\r\nintroduced** by refactoring the project over time.\r\n\r\nThey enable **avoiding repetition**, **facilitating evolution** of code more\r\neasily, **improving** it, and especially **better testing**.\r\n\r\nIn this article, we have seen how **certain models** like\r\n**Singleton**, **Factory Method**, or **Facade** can be applied\r\n**directly** in a modern architecture like React + Zustand. Other\r\nmore **conceptual** patterns (Builder, Strategy, Decorator) were illustrated\r\nin **pseudo-code** form to better grasp their intention.\r\n\r\n### In Summary:\r\n\r\n- Patterns are not constraints but **mastered freedom**.\r\n- They enable **avoiding classic pitfalls** of development as projects expand.\r\n- **Learning to recognize** and utilize these models also means progressing in **software maturity**.","src/data/blog/en/qu-est-ce-que-design-pattern.mdx","637821426cdac165"]